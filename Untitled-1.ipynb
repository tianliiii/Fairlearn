{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from FeatBoost.feat_selector import FeatureSelector\n",
    "import json\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "import json\n",
    "import csv\n",
    "import uncertainty\n",
    "from scipy.stats import beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4086e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort = pd.read_parquet(\"./DataProcessing/Sep_24_gim_icd10.parquet\")\n",
    "sbk_gim = pd.read_parquet(\"./DataProcessing/Sep_24_sbk_gim_icd10.parquet\")\n",
    "\n",
    "non_gim_cohort = pd.read_parquet(\"./DataProcessing/Sep_24_non_gim_icd10.parquet\")\n",
    "locality = pd.read_csv(\"./fair_interpretable/fair_inter_locality_v2_update.csv\")\n",
    "statcan = pd.read_csv('./fair_interpretable/statcan_table.csv')\n",
    "zero_sum_columns = [col for col in gim_cohort.columns if gim_cohort[col].sum() == 0]\n",
    "gim_cohort = gim_cohort.drop(columns=zero_sum_columns)\n",
    "sbk_gim = sbk_gim.drop(columns=zero_sum_columns)\n",
    "non_gim_cohort = non_gim_cohort.drop(columns=zero_sum_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness = locality.merge(statcan, how = 'left', on = 'da21uid')\n",
    "print(fairness.shape)\n",
    "fairness = fairness[['genc_id', 'households_dwellings_q_DA21','material_resources_q_DA21','age_labourforce_q_DA21','racialized_NC_pop_q_DA21']]\n",
    "fairness.columns = ['genc_id', 'households_dwellings', 'material_resources', 'age_labourforce', 'racialized']\n",
    "del locality\n",
    "del statcan\n",
    "gc.collect()\n",
    "fairness_columns = list(fairness.columns)[1:]\n",
    "print(fairness_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort = gim_cohort.drop_duplicates()\n",
    "gim_cohort = gim_cohort.reset_index(drop=True)\n",
    "gim_cohort = pd.concat([gim_cohort, sbk_gim], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351eb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_rate(y, fairness_features_df, culmulative=False):\n",
    "    \"\"\"\n",
    "    Calculate prevalence rate of delirium across fairness feature groups\n",
    "    \"\"\"\n",
    "    prevalence_rates = []\n",
    "    for fairness_feature in fairness_features_df.columns:\n",
    "        if culmulative:\n",
    "            if fairness_feature == \"gender_F\":\n",
    "                splits = [0]\n",
    "            else:\n",
    "                splits = [1,2,3,4]\n",
    "            for split in splits:\n",
    "                fairness_binary = (fairness_features_df[fairness_feature]>split).astype(int)\n",
    "                group0_mask = fairness_binary == 0\n",
    "                group1_mask = fairness_binary == 1\n",
    "               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dd7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk_binary(y_true, y_pred_probs, k):\n",
    "    \"\"\"\n",
    "    Computes the Average Precision at K (AP@K) for binary predictions.\n",
    "    :param y_true: List or array of ground truth binary labels (0 or 1).\n",
    "    :param y_pred_probs: List or array of predicted probabilities.\n",
    "    :param k: The number of top predictions to consider.\n",
    "    :return: Average Precision at K (AP@K).\n",
    "    \"\"\"\n",
    "    # Sort predictions by predicted probability in descending order\n",
    "    sorted_indices = np.argsort(y_pred_probs)[::-1]\n",
    "    y_true_sorted = np.array(y_true)[sorted_indices]\n",
    "\n",
    "    # Compute precision at each relevant position\n",
    "    num_hits = 0.0\n",
    "    score = 0.0\n",
    "\n",
    "    for i in range(min(k, len(y_true_sorted))):\n",
    "        if y_true_sorted[i] == 1:  # Relevant item\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)  # Precision at position i+1\n",
    "\n",
    "    # Normalize by the number of relevant items or k\n",
    "    return score / min(sum(y_true), k) if sum(y_true) > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def calc_metrics(prediction, prediction_prob, labels, k=10):\n",
    "    acc = accuracy_score(labels, prediction)\n",
    "    f1 = f1_score(labels, prediction, average='binary')  # Use 'micro', 'macro', 'weighted' for multi-class\n",
    "    precision = precision_score(labels, prediction, average='binary')\n",
    "    recall = recall_score(labels, prediction, average='binary')\n",
    "    roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "    # Precision@k calculation\n",
    "    # Sort by prediction probabilities in descending order\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    \n",
    "    # Count true positives in the top k predictions\n",
    "    top_k_labels = np.array(labels)[top_k_indices]\n",
    "    precision_at_k = np.sum(top_k_labels) / k\n",
    "    \n",
    "    return acc, f1, precision, recall, roc_auc, precision_at_k\n",
    "    # return acc, f1, precision, recall, roc_auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_group_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute TPR, FPR, Precision, and Recall for a given group.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else np.nan\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "    recall = TPR  # Recall is the same as TPR\n",
    "    \n",
    "    return TPR, FPR, precision, recall\n",
    "\n",
    "\n",
    "def calc_metrics_at_thresholds(num_features, prediction_prob, labels, fairness_features_df, thresholds=[0.5], k=10, culmulative=False):\n",
    "    \"\"\" \n",
    "    Compute the fairness score at different thresholds at different split, and different fairness features\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Calculate the ROC AUC once, as it does not depend on the threshold\n",
    "    roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "    # Calculate Precision@k\n",
    "    # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    # Create a mask for the top k predictions\n",
    "    top_k_mask = np.zeros(len(labels), dtype=bool)\n",
    "    top_k_mask[top_k_indices] = True\n",
    "\n",
    "    top_k_labels = np.array(labels)[top_k_indices]\n",
    "    precision_at_k = np.sum(top_k_labels) / k\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Apply the threshold to generate predictions\n",
    "        prediction = (prediction_prob >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        correct_prediction = labels == prediction\n",
    "        acc = np.mean(correct_prediction)\n",
    "        f1 = f1_score(labels, prediction, average='binary')\n",
    "        precision = precision_score(labels, prediction, average='binary')\n",
    "        recall = recall_score(labels, prediction, average='binary')\n",
    "        map_k = apk_binary(labels, prediction_prob, k=labels.shape[0])\n",
    "        \n",
    "        # Calculate the percentage of positive predictions\n",
    "        pred_positive_percentage = np.mean(prediction) \n",
    "        true_positive_percentage = np.mean(labels)\n",
    "        \n",
    "        for fairness_feature in list(fairness_features_df.columns):\n",
    "            if culmulative:\n",
    "                ## Culmulative split points for fairness features\n",
    "                if fairness_feature == \"gender_F\":\n",
    "                    splits = [0]\n",
    "                else:\n",
    "                    splits = [1,2,3,4]\n",
    "                col_name_split = \"culmulative_split_point(<= split)\"\n",
    "                \n",
    "            else:\n",
    "                # exact split points for fairness features\n",
    "                splits = fairness_features_df[fairness_feature].unique()\n",
    "                col_name_split = \"exact_split_point(== split)\"\n",
    "            for split in splits:\n",
    "\n",
    "                if culmulative:\n",
    "                    # Convert the fairness feature to binary based culmulative split points ex. (<= split) -> 1 , (> split) -> 0\n",
    "                    fairness_binary = (fairness_features_df[fairness_feature]<=split).astype(int)\n",
    "                else:\n",
    "                    # Convert the fairness feature to binary based exat split points ex.(== split) -> 1, (!= split) -> 0\n",
    "                    fairness_binary = (fairness_features_df[fairness_feature] == split).astype(int)\n",
    "                group0_mask = fairness_binary == 0\n",
    "                group1_mask = fairness_binary == 1\n",
    "\n",
    "                ### prevalence rate\n",
    "                ##“How common is label = 1 in this group in the real world?”\tLarge natural imbalance → model must handle different base-rates.\n",
    "                prevalence_rate1 = (labels[group1_mask].sum() / len(labels[group1_mask])) if len(labels[group1_mask]) > 0 else 0\n",
    "                prevalence_rate0 = (labels[group0_mask].sum() / len(labels[group0_mask])) if len(labels[group0_mask]) > 0 else 0\n",
    "\n",
    "\n",
    "                # calculate prevalence rate of delirium across fairness feature groups @ the top k predictions\n",
    "                #“Among the group’s members who made the shortlist (top-k), what fraction truly need/deserve the action?” (precision of ranking within the group)\t\n",
    "                # One group’s Prev@k ≫ another’s → scarce slots for the second group are ‘wasted’ on false positives or its true positives are being outranked.\n",
    "                prevalence_rate1_k = (labels[group1_mask&top_k_mask].sum() / len(labels[group1_mask&top_k_mask])) if len(labels[group1_mask&top_k_mask]) > 0 else 0\n",
    "                prevalence_rate0_k = (labels[group0_mask&top_k_mask].sum() / len(labels[group0_mask&top_k_mask])) if len(labels[group0_mask&top_k_mask]) > 0 else 0\n",
    "\n",
    "                ### treatment rate\n",
    "                # “With my chosen threshold, how often do I give the positive decision to this group?”\t\n",
    "                # #Gap ≫ prevalence gap → model amplifies imbalance; gap ≪ prevalence gap → model may under-serve high-need group.\n",
    "                treatment_rate1 = (prediction[group1_mask].sum() / len(prediction[group1_mask])) if len(prediction[group1_mask]) > 0 else 0\n",
    "                treatment_rate0 = (prediction[group0_mask].sum() / len(prediction[group0_mask])) if len(prediction[group0_mask]) > 0 else 0\n",
    "\n",
    "                # calculate treatment rate of delirium across fairness feature groups @ the top k predictions\n",
    "                # “What share of the entire group lands in the topk?” (allocation of a limited resource)\t\n",
    "                # TR@k gap shows direct disparate opportunity or burden when capacity is capped \n",
    "                treatment_rate1_k = (group1_mask & top_k_mask).sum() / group1_mask.sum() if group1_mask.sum() > 0 else 0\n",
    "                treatment_rate0_k = (group0_mask & top_k_mask).sum() / group0_mask.sum() if group0_mask.sum() > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "                if sum(group0_mask) == 0 or sum(group1_mask) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                group0_labels = np.array(labels)[group0_mask]\n",
    "                group0_preds = np.array(prediction)[group0_mask]\n",
    "    \n",
    "                group1_labels = np.array(labels)[group1_mask]\n",
    "                group1_preds = np.array(prediction)[group1_mask]\n",
    "\n",
    "                TPR_0, FPR_0, precision_0, recall_0 = compute_group_metrics(group0_labels, group0_preds)\n",
    "                TPR_1, FPR_1, precision_1, recall_1 = compute_group_metrics(group1_labels, group1_preds)\n",
    "\n",
    "                tpr_diff = TPR_1 - TPR_0\n",
    "                fpr_diff = FPR_1 - FPR_0\n",
    "                prec_diff = precision_1 - precision_0\n",
    "                rec_diff = recall_1 - recall_0\n",
    "\n",
    "                # Calculate the tpr, fpr, precision, recall  @k\n",
    "                TPR_0_k, FPR_0_k, precision_0_k, recall_0_k = compute_group_metrics(labels[group0_mask], # y_true  (only group-0 rows)\n",
    "                                                                                    top_k_mask[group0_mask]) # y_pred  (1 if that row is in the top-k)\n",
    "                TPR_1_k, FPR_1_k, precision_1_k, recall_1_k = compute_group_metrics(labels[group1_mask], \n",
    "                                                                                    top_k_mask[group1_mask])\n",
    "                tpr_diff_k = TPR_1_k - TPR_0_k\n",
    "                fpr_diff_k = FPR_1_k - FPR_0_k\n",
    "                prec_diff_k = precision_1_k - precision_0_k\n",
    "                rec_diff_k = recall_1_k - recall_0_k\n",
    "\n",
    "\n",
    "\n",
    "                ##### Bayesian Unfairness and Uncertainty\n",
    "                # Setting correct prediction as the favorable outcome, Bayesian disparity assumes both groups have a 50% chance of receiving the favorable outcome.\n",
    "                E1 = correct_prediction \n",
    "                E2 = True\n",
    "                bayesian_disparity = uncertainty.bayesian_disparity(group0_mask, group1_mask, E1, E2)\n",
    "                bayesian_disparity_abs = np.abs(bayesian_disparity)\n",
    "                uncertainty_value = uncertainty.uncertainty(group0_mask, group1_mask, E1, E2)\n",
    "\n",
    "\n",
    "                # Calculate the uncertainty value@k\n",
    "                E1_k = np.array(labels)[top_k_mask] == prediction[top_k_mask]\n",
    "                E2_k = True\n",
    "                bayesian_disparity_k = uncertainty.bayesian_disparity(group1_mask[top_k_mask], group0_mask[top_k_mask], E1_k, E2_k)\n",
    "                bayesian_disparity_abs_k = np.abs(bayesian_disparity_k)\n",
    "                uncertainty_value_k = uncertainty.uncertainty(group0_mask[top_k_mask], group1_mask[top_k_mask], E1_k, E2_k)\n",
    "\n",
    "                \n",
    "    \n",
    "                results.append({\n",
    "                        'num_features': num_features,\n",
    "                        'threshold': threshold,\n",
    "                        'fairness_feature': fairness_feature,\n",
    "                        col_name_split: split,\n",
    "                        'group_0_size': int((fairness_binary == 0).sum()),\n",
    "                        'group_1_size': int((fairness_binary == 1).sum()),\n",
    "                        'accuracy': acc,\n",
    "                        'f1_score': f1,\n",
    "                        'precision': precision,                        \n",
    "                        'recall': recall,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'precision_at_k': precision_at_k,\n",
    "                        'map@k': map_k,\n",
    "                        'pred_positive_percentage': pred_positive_percentage,\n",
    "                        'true_positive_percentage': true_positive_percentage,\n",
    "                        'tpr_diff_abs': abs(tpr_diff),\n",
    "                        'fpr_diff_abs': abs(fpr_diff),\n",
    "                        'tpr_diff_raw': tpr_diff,\n",
    "                        'fpr_diff_raw': fpr_diff,\n",
    "                        'precision_diff_abs': abs(prec_diff),\n",
    "                        'recall_diff_abs': abs(rec_diff),\n",
    "                        'equalized_odds_max': max(abs(tpr_diff), abs(fpr_diff)),\n",
    "                        'equalized_odds': 0.5*(abs(tpr_diff) + abs(fpr_diff)),\n",
    "                        'bayesian_disparity': bayesian_disparity,\n",
    "                        'bayesian_disparity_abs':bayesian_disparity_abs,\n",
    "                        'bayesian_uncertainty': uncertainty_value,\n",
    "                        'prevalence_rate1': prevalence_rate1,\n",
    "                        'prevalence_rate0': prevalence_rate0,\n",
    "                        'treatment_rate1': treatment_rate1,\n",
    "                        'treatment_rate0': treatment_rate0,\n",
    "                        'prevalence_rate1@k': prevalence_rate1_k,\n",
    "                        'prevalence_rate0@k': prevalence_rate0_k,\n",
    "                        'treatment_rate1@k': treatment_rate1_k,\n",
    "                        'treatment_rate0@k': treatment_rate0_k,\n",
    "                        'tpr_0@k': TPR_0_k,\n",
    "                        'fpr_0@k': FPR_0_k,\n",
    "                        'precision_0@k': precision_0_k,\n",
    "                        'recall_0@k': recall_0_k,\n",
    "                        'tpr_1@k': TPR_1_k,\n",
    "                        'fpr_1@k': FPR_1_k,\n",
    "                        'precision_1@k': precision_1_k,\n",
    "                        'recall_1@k': recall_1_k,\n",
    "                        'tpr_diff@k': tpr_diff_k,\n",
    "                        'fpr_diff@k': fpr_diff_k,\n",
    "                        'precision_diff@k': prec_diff_k,\n",
    "                        'recall_diff@k': rec_diff_k,\n",
    "                        'bayesian_disparity@k': bayesian_disparity_k,\n",
    "                        'bayesian_disparity_abs@k': bayesian_disparity_abs_k,\n",
    "                        'bayesian_uncertainty@k': uncertainty_value_k,\n",
    "                        'equalized_odds_max@k': max(abs(tpr_diff_k), abs(fpr_diff_k)),\n",
    "                        'equalized_odds@k': 0.5*(abs(tpr_diff_k) + abs(fpr_diff_k))\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    # max_eod_df = df.loc[df.groupby('fairness_feature')['equalized_odds'].idxmax()]\n",
    "            \n",
    "            \n",
    "                    \n",
    "\n",
    " \n",
    "            \n",
    "    return df #max_eod_df\n",
    "\n",
    "\n",
    "def bootstrap_sample(data, proportion=0.8):\n",
    "    \"\"\"\n",
    "    Bootstrap sampling function.\n",
    "    :param data: DataFrame to sample from.\n",
    "    :param proportion: Proportion of the data to sample.\n",
    "    :return: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    n_samples = int(len(data) * proportion)\n",
    "    return data.sample(n_samples, replace=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dist_plot_top_k(prediction_prob, fairness_feature, k=10):\n",
    "    # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_fairness_feature = np.array(fairness_feature)[top_k_indices]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the distribution of the top k predictions\n",
    "    ax[0].hist(prediction_prob[top_k_indices], bins=20, color='skyblue', edgecolor='black')\n",
    "    ax[0].set_title(f'Distribution of Top {k} Predictions')\n",
    "    ax[0].set_xlabel('Prediction Probability')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    \n",
    "    # Plot the distribution of the fairness feature in the top k predictions\n",
    "    ax[1].hist(top_k_fairness_feature, bins=20, color='lightcoral', edgecolor='black')\n",
    "    ax[1].set_title(f'Distribution of Fairness Feature in Top {k} Predictions')\n",
    "    ax[1].set_xlabel('Fairness Feature Value')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_sample(data, proportion=0.8):\n",
    "    \"\"\"\n",
    "    Bootstrap sampling function.\n",
    "    :param data: DataFrame to sample from.\n",
    "    :param proportion: Proportion of the data to sample.\n",
    "    :return: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    n_samples = int(len(data) * proportion)\n",
    "    return data.sample(n_samples, replace=True, random_state=42)\n",
    "\n",
    "def bootstrap_confidence_interval(data, metric_func, n_iterations=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a metric using bootstrap sampling.\n",
    "    :param data: DataFrame containing the data.\n",
    "    :param metric_func: Function to calculate the metric.\n",
    "    :param n_iterations: Number of bootstrap iterations.\n",
    "    :param alpha: Significance level for the confidence interval.\n",
    "    :return: Tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "    for i in range(n_iterations):\n",
    "        sample = bootstrap_sample(data)\n",
    "        metric_df  = metric_func(sample)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4da191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def rND(prediction_prob,\n",
    "        fairness_feature_df,\n",
    "        target_fairness_feature='gender_F',\n",
    "        protected_val=1,\n",
    "        rank_k=250,\n",
    "        diretional=False):\n",
    "    \"\"\"\n",
    "    Rank-aware Normalised Discounted Difference (rND).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_prob : 1-D array-like, length n\n",
    "        Scores or probabilities; higher = higher rank.\n",
    "    fairness_feature_df : pandas.DataFrame, shape (n, m)\n",
    "        DataFrame holding the sensitive attributes.\n",
    "    target_fairness_feature : str, default 'gender_f'\n",
    "        Column that marks protected items (binary).\n",
    "    protected_val : int , default 1 -> female\n",
    "        The value that identifies membership in the protected group.\n",
    "    rank_k : int, default 250\n",
    "        Largest cut-off K to examine .\n",
    "    \n",
    "    diretional : bool, default False\n",
    "        If True, returns two scores: one for over-representation and one for under-representation.\n",
    "        If False, returns a single unsigned rND score.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        rND in the range [0, 1].  0 = perfect proportionality,\n",
    "        1 = maximally skewed for the given data.\n",
    "        if diretional = True:\n",
    "        return (over_repr, under_repr)\n",
    "    \"\"\"\n",
    "    N = len(prediction_prob)\n",
    "    rank_k = min(rank_k, N)              \n",
    "    k_list = list(range(10, rank_k + 1, 10))  # 10, 20, 30, …\n",
    "\n",
    "    ranked_idx = np.argsort(prediction_prob)[::-1]  \n",
    "\n",
    "    # Pre-compute global prevalence\n",
    "    prot_mask = (fairness_feature_df[target_fairness_feature] == protected_val).values\n",
    "    g = prot_mask.sum()                   # |G₁|\n",
    "    if g == 0:\n",
    "        raise ValueError(\"# in protected group is zero.\")  \n",
    "    p_global = g / N                      # prevalence in the full population\n",
    "\n",
    "    # loop through each cut-off K\n",
    "    raw_score = 0.0\n",
    "    if diretional:\n",
    "        raw_score_over_repr = 0.0\n",
    "        raw_score_under_repr = 0.0\n",
    "    for k in k_list:\n",
    "        k_indices = ranked_idx[:k]        # indices of the top-k prefix\n",
    "        p_topk = prot_mask[k_indices].sum() / k   # proportion of G₁ in top-k\n",
    "\n",
    "        raw_score += abs(p_topk - p_global) / np.log2(k)\n",
    "        if diretional:\n",
    "            raw_score_over_repr += max(0, p_topk - p_global) / np.log2(k)\n",
    "            raw_score_under_repr += max(0, p_global - p_topk) / np.log2(k)\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #    Normaliser Z  (worst-case raw rND)\n",
    "    #    We compute it for the two extreme rankings:\n",
    "    #    A) all protected items first, B) all protected items last,\n",
    "    #    and keep the larger magnitude.\n",
    "    # ------------------------------------------------------------------\n",
    "    Z_top = Z_bottom = 0.0\n",
    "    if diretional:                   # if we want to compute over- and under-representation difference separately\n",
    "        Z_top_over_repr = Z_top_under_repr = Z_bottom_over_repr = Z_bottom_under_repr = 0.0\n",
    "    for k in k_list:                       # 10, 20, 30, …\n",
    "        # prevalence if ALL protected items are at the top\n",
    "        prop_top = min(g, k) / k\n",
    "        Z_top += abs(prop_top - p_global) / np.log2(k)\n",
    "    \n",
    "        # prevalence if ALL protected items are at the bottom\n",
    "        prop_bottom = max(0, g - (N - k)) / k\n",
    "        Z_bottom += abs(prop_bottom - p_global) / np.log2(k)\n",
    "\n",
    "        if diretional:\n",
    "            Z_top_over_repr += max(0, prop_top - p_global) / np.log2(k)\n",
    "            Z_top_under_repr += max(0, p_global - prop_top) / np.log2(k)\n",
    "            Z_bottom_over_repr += max(0, prop_bottom - p_global) / np.log2(k)\n",
    "            Z_bottom_under_repr += max(0, p_global - prop_bottom) / np.log2(k)\n",
    "    \n",
    "    Z = max(Z_top, Z_bottom)               # highest possible rND\n",
    "\n",
    "        \n",
    "    if Z == 0:            # happens only if no protected items exist\n",
    "        return 0.0\n",
    "    \n",
    "    if diretional:\n",
    "        Z_over_repr = max(Z_top_over_repr, Z_bottom_over_repr)\n",
    "        Z_under_repr = max(Z_top_under_repr, Z_bottom_under_repr)\n",
    "        if Z_over_repr == 0 or Z_under_repr == 0:\n",
    "            return 0.0\n",
    "        return raw_score_over_repr / Z_over_repr, raw_score_under_repr / Z_under_repr\n",
    "    return raw_score / Z            # unsigned rND  ∈ [0, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rRD(prediction_prob,\n",
    "        fairness_feature_df,\n",
    "        target_fairness_feature='gender_F',\n",
    "        protected_val=1,\n",
    "        rank_k=250):\n",
    "    \n",
    "    \"\"\"\n",
    "    Rank-aware Normalised Discounted Difference (rRD).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_prob : 1-D array-like, length n\n",
    "        Scores or probabilities; higher = higher rank.\n",
    "    fairness_feature_df : pandas.DataFrame, shape (n, m)\n",
    "        DataFrame holding the sensitive attributes.\n",
    "    target_fairness_feature : str, default 'gender_f'\n",
    "        Column that marks protected items (binary).\n",
    "    protected_val : int, if 1 then female/male, if 0 then male/female ratio\n",
    "        The value that identifies membership in the protected group.\n",
    "    rank_k : int, default 250\n",
    "        Largest cut-off K to examine .\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        rRD in the range [0, 1].  0 = perfect proportionality,\n",
    "        1 = maximally skewed for the given data.\n",
    "    \"\"\"\n",
    " \n",
    "    N = len(prediction_prob)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"Empty prediction array\")\n",
    "\n",
    "    rank_k = min(rank_k, N)\n",
    "    k_list = list(range(10, rank_k + 1, 10))\n",
    "\n",
    "  \n",
    "    ranked_idx = np.argsort(prediction_prob)[::-1]\n",
    "\n",
    "    prot_mask = (fairness_feature_df[target_fairness_feature] == protected_val).values\n",
    "    unprot_mask = ~prot_mask\n",
    "\n",
    "    g1 = int(prot_mask.sum())             # |S⁺|\n",
    "    g2 = int(unprot_mask.sum())           # |S⁻|\n",
    "\n",
    "    if g1 == 0 or g2 == 0:\n",
    "        raise ValueError(\"g1 and g2 cannot be zero.\")          # rRD undefined\n",
    "\n",
    "    global_ratio = g1 / g2                # |S⁺| / |S⁻|\n",
    "\n",
    "  \n",
    "    raw_score = 0.0\n",
    "    for k in k_list:\n",
    "        k_indices = ranked_idx[:k]\n",
    "        prot_topk   = int(prot_mask[k_indices].sum())\n",
    "        unprot_topk = k - prot_topk       \n",
    "\n",
    "        # if numerator or denominator is 0 -> that term = 0\n",
    "        if prot_topk == 0 or unprot_topk == 0:\n",
    "            raw_score += abs(-global_ratio) / np.log2(k)\n",
    "            continue\n",
    "\n",
    "        prefix_ratio = prot_topk / unprot_topk\n",
    "        raw_score += abs(prefix_ratio - global_ratio) / np.log2(k)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #    Normaliser Z  (worst-case raw rRD)\n",
    "    #    We compute it for the two extreme rankings:\n",
    "    #    A) all protected items first, B) all protected items last,\n",
    "    #    and keep the larger magnitude.\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    Z_top = Z_bottom = 0.0\n",
    "    for k in k_list:                       # 10, 20, 30, …\n",
    "        \n",
    "        # prevalence if ALL protected items are at the top\n",
    "        prot_top = min(k,g1)\n",
    "        unprot_top = k - prot_top\n",
    "        \n",
    "        if prot_top == 0 or unprot_top == 0:\n",
    "            Z_top += abs(-global_ratio) / np.log2(k)\n",
    "            \n",
    "        else: Z_top += abs(prot_top/unprot_top - global_ratio) / np.log2(k)\n",
    "\n",
    "        # prevalence if ALL protected items are at the bottom\n",
    "        prot_bot = max(0, k-g2)\n",
    "        unprot_bot = k - prot_bot\n",
    "\n",
    "        if prot_bot == 0 or unprot_bot == 0:\n",
    "            Z_bottom += abs(-global_ratio) / np.log2(k)\n",
    "\n",
    "        else: Z_bottom += abs(prot_bot / unprot_bot - global_ratio) / np.log2(k)\n",
    "\n",
    "\n",
    "    Z = max(Z_top, Z_bottom)               # highest possible rND\n",
    "\n",
    "        \n",
    "    if Z == 0:                            # happens only if no protected items exist\n",
    "        return 0.0\n",
    "    return raw_score / Z            # unsigned rND  ∈ [0, 1]\n",
    "\n",
    "\n",
    "\n",
    "def rKL(prediction_prob,\n",
    "        fairness_feature_df,\n",
    "        target_fairness_feature,\n",
    "        rank_k=250):\n",
    "    \"\"\"\n",
    "    Rank-aware Normalised Kullback-Leibler Divergence (rKL).\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_prob : 1-D array-like, length n\n",
    "        Scores or probabilities; higher = higher rank.\n",
    "    fairness_feature_df : pandas.DataFrame, shape (n, m)\n",
    "        DataFrame holding the sensitive attributes.\n",
    "    target_fairness_feature : str\n",
    "        Column that marks protected items (can be binary or multinary).\n",
    "    rank_k : int, default 250\n",
    "        Largest cut-off K to examine .\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        rKL in the range [0, 1].  0 = perfect proportionality,\n",
    "        1 = maximally skewed for the given data.\n",
    "    \"\"\"\n",
    "    eps = 1e-12  # small value to avoid division by zero\n",
    "    N = len(prediction_prob)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"Empty prediction array\")\n",
    "    rank_k = min(rank_k, N)\n",
    "    k_list = list(range(10, rank_k + 1, 10))  # 10, 20, 30, …\n",
    "    ranked_idx = np.argsort(prediction_prob)[::-1]  # indices of the top-k prefix\n",
    "    fairness_col = fairness_feature_df[target_fairness_feature]      # keep as Series\n",
    "    categories = [c for c in fairness_col.dropna().unique()]\n",
    "    fairness_feature_array = fairness_col.to_numpy()  # convert to numpy array for indexing\n",
    "\n",
    "\n",
    "\n",
    "    if len(categories) < 2:\n",
    "        raise ValueError(\"At least two categories are required for rKL calculation.\")\n",
    "    if target_fairness_feature != \"gender_F\" and len(categories) !=5:\n",
    "        print(f\"Warning: {target_fairness_feature} has {len(categories)} categories, expected 5 for fairness analysis.\")\n",
    "\n",
    "    Q = []\n",
    "    for category in categories:\n",
    "        mask = fairness_feature_array == category\n",
    "        percent = np.sum(mask) / N  # proportion of each category in the full population\n",
    "        Q.append(percent)  # proportions of each category in the full population\n",
    "    Q = np.array(Q)\n",
    "    Q = np.clip(Q, eps, 1.0)  # ensure no zero probabilities\n",
    "\n",
    "    \n",
    "\n",
    "    def raw_rKL_calculator(fairness_feature_array, ranked_idx):\n",
    "        \"\"\"\n",
    "        Calculate the Kullback-Leibler divergence between two probability distributions P and Q.\n",
    "        \"\"\"\n",
    "        # loop through each cut-off K\n",
    "        raw_rkl = 0.0\n",
    "        for k in k_list:\n",
    "            k_indices = ranked_idx[:k]\n",
    "            mask = fairness_feature_array[k_indices]  # fairness feature values for the top-k prefix\n",
    "            counts = np.array([(mask==category).sum() for category in categories])\n",
    "            P = counts / k\n",
    "            P = np.clip(P, eps, 1.0)  # ensure no zero\n",
    "        \n",
    "            KL_k = np.sum(P * np.log(P / Q)) / np.log2(k)  # Kullback-Leibler divergence for the top-k prefix\n",
    "            raw_rkl += KL_k\n",
    "        return raw_rkl\n",
    "\n",
    "    raw_rkl = raw_rKL_calculator(fairness_feature_array, ranked_idx)\n",
    "\n",
    "\n",
    "    #---------------------------------------------\n",
    "    #    Normaliser Z  (worst-case raw rKL)\n",
    "    #    We compute it for the all extreme permutations\n",
    "    #    of the fairness feature categories.\n",
    "    #    For each permutation, we calculate the Kullback-Leibler divergence\n",
    "    #    and keep the larger magnitude.\n",
    "    #-------------------------------------------------\n",
    "    import itertools\n",
    "    idx_by_cat = {c: np.where(fairness_feature_array == c)[0] for c in categories} \n",
    "    Z = []\n",
    "    for perm in itertools.permutations(categories):\n",
    "        permuted_indices = np.concatenate([idx_by_cat[c] for c in perm])\n",
    "        Z.append(raw_rKL_calculator(fairness_feature_array, permuted_indices))\n",
    "    Z = max(Z)  # highest possible rKL\n",
    "    if Z == 0:  # happens only if no protected items exist\n",
    "        return 0.0\n",
    "    return raw_rkl / Z  # unsigned rKL ∈ [0, 1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2b4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk_binary(y_true, y_pred_probs, k):\n",
    "    \"\"\"\n",
    "    Computes the Average Precision at K (AP@K) for binary predictions.\n",
    "    :param y_true: List or array of ground truth binary labels (0 or 1).\n",
    "    :param y_pred_probs: List or array of predicted probabilities.\n",
    "    :param k: The number of top predictions to consider.\n",
    "    :return: Average Precision at K (AP@K).\n",
    "    \"\"\"\n",
    "    # Sort predictions by predicted probability in descending order\n",
    "    sorted_indices = np.argsort(y_pred_probs)[::-1]\n",
    "    y_true_sorted = np.array(y_true)[sorted_indices]\n",
    "\n",
    "    # Compute precision at each relevant position\n",
    "    num_hits = 0.0\n",
    "    score = 0.0\n",
    "\n",
    "    for i in range(min(k, len(y_true_sorted))):\n",
    "        if y_true_sorted[i] == 1:  # Relevant item\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)  # Precision at position i+1\n",
    "\n",
    "    # Normalize by the number of relevant items or k\n",
    "    return score / min(sum(y_true), k) if sum(y_true) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Define Parameter Grid for XGBoost\n",
    "# =============================================================================\n",
    "X_train = np.load(\"./data/X_train.npy\")\n",
    "y_train = np.load(\"./data/y_train.npy\")\n",
    "X_test = np.load(\"./data/X_test.npy\")\n",
    "y_test = np.load(\"./data/y_test.npy\")\n",
    "fairness_features_train = pd.read_csv(\"./data/fairness_features_train.csv\")\n",
    "fairness_features_test = pd.read_csv(\"./data/fairness_features_test.csv\")\n",
    "\n",
    "\n",
    "grid_search_results_path = \"./experiment_result/grid_search_apk_rKL.csv\"\n",
    "pareto_frontier_param_path = \"./experiment_result/pareto_frontier_param_apk_rKL.json\"\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200],  # Number of boosting rounds\n",
    "    'learning_rate': [0.1, 0.3, 0.05],  # Step size shrinkage\n",
    "    'max_depth': [3, 5],  # Maximum depth of a tree\n",
    "    'subsample': [1.0, 0.7],  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [1.0, 0.7],  # Subsample ratio of columns\n",
    "    'alpha': [1.0, 5.0],\n",
    "    'reg_lambda': [2.0, 5.0],\n",
    "    'scale_pos_weight': [10.0, 15.0]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Grid Search with 3-Fold CV (Optimizing AUC and Fairness)\n",
    "# =============================================================================\n",
    "def cross_validate_and_grid_search(X_train, y_train, X_test, y_test, fairness_features_train, fairness_features_test,\n",
    "                               param_grid, target_fairness_feature, rank_k, n_folds, grid_search_results_path, pareto_frontier_param_path):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation to optimize rKL and selected fairness metrics.\n",
    "    :param X_train: Training features.\n",
    "    :param y_train: Training labels.\n",
    "    :param X_test: Test features.\n",
    "    :param y_test: Test labels.\n",
    "    :param fairness_features_train: Fairness features for training set.\n",
    "    :param fairness_features_test: Fairness features for test set.\n",
    "    :param param_grid: Dictionary of hyperparameters to search.\n",
    "    :param target_fairness_feature: The fairness feature to optimize (e.g., 'gender_F').\n",
    "    :param rank_k: The number of top predictions to consider for fairness metrics. rKL will be computed on the top [10, 20, ..., rank_k] predictions.\n",
    "    :param n_folds: Number of folds for cross-validation.\n",
    "    :param grid_search_results_path: Path to save grid search results.\n",
    "    :param pareto_frontier_param_path: Path to save Pareto frontier parameters.\n",
    "    :return: DataFrame of results.\n",
    "    \"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "    for params in grid:\n",
    "        # Lists to store fold-level metrics\n",
    "        fold_cv_rKL = []\n",
    "        fold_cv_apk = []\n",
    "        fold_train_rKL = []\n",
    "        fold_train_apk = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "            fairness_tr = fairness_features_train.iloc[train_idx]\n",
    "            fairness_val = fairness_features_train.iloc[val_idx]\n",
    "            \n",
    "            # Initialize and train model using current parameter set\n",
    "            model = xgb.XGBClassifier(\n",
    "                **params,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "            model.fit(X_tr, y_tr)\n",
    "            #For training set\n",
    "            y_tr_pred_prob = model.predict_proba(X_tr)[:, 1]\n",
    "            # Compute rKL on training set\n",
    "            rKL_tr = rKL(y_tr_pred_prob, fairness_tr, target_fairness_feature, rank_k=rank_k)\n",
    "            # Compute apk on training set\n",
    "            apk_tr = apk_binary(y_tr, y_tr_pred_prob, k=rank_k)   \n",
    "            \n",
    "            fold_train_rKL.append(rKL_tr)\n",
    "            fold_train_apk.append(apk_tr)\n",
    "\n",
    "            # For CV, get predictions and probabilities on validation set\n",
    "            y_val_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Compute AUC on validation set\n",
    "            rKL_val = rKL(y_val_pred_prob, fairness_val, target_fairness_feature, rank_k=rank_k)\n",
    "            apk_val = apk_binary(y_val, y_val_pred_prob, k=rank_k)\n",
    "        \n",
    "            # Append metrics for this fold\n",
    "            fold_cv_rKL.append(rKL_val)\n",
    "            fold_cv_apk.append(apk_val)\n",
    "        \n",
    "        # Average metrics across CV and train folds\n",
    "        avg_cv_rKL = np.mean(fold_cv_rKL)\n",
    "        avg_cv_apk = np.mean(fold_cv_apk)\n",
    "        avg_train_rKL = np.mean(fold_train_rKL)\n",
    "        avg_train_apk = np.mean(fold_train_apk)\n",
    "\n",
    "        # Retrain the model on the full training set with the given parameters\n",
    "        final_model = xgb.XGBClassifier(\n",
    "            **params,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        final_model.fit(X_train, y_train)\n",
    "        y_test_pred_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Compute test metrics\n",
    "        test_rKL = rKL(y_test_pred_prob, fairness_features_test, target_fairness_feature, rank_k=rank_k)\n",
    "        test_apk = apk_binary(y_test, y_test_pred_prob, k=rank_k)\n",
    "        \n",
    "        # Save all metrics along with the parameter settings\n",
    "        record = {\n",
    "            'params': params,\n",
    "            'cv_rKL': avg_cv_rKL,\n",
    "            'cv_apk': avg_cv_apk,\n",
    "            'train_rKL': avg_train_rKL,\n",
    "            'train_apk': avg_train_apk,\n",
    "            'test_rKL': test_rKL,\n",
    "            'test_apk': test_apk,\n",
    "        }\n",
    "\n",
    "        # Append the record to the results CSV\n",
    "        with open(grid_search_results_path, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=record.keys())\n",
    "            \n",
    "            # Write the header only if the file is new\n",
    "            if file.tell() == 0:  # Check if file is empty\n",
    "                writer.writeheader()\n",
    "            writer.writerow(record)\n",
    "        results.append(record)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Compute the Pareto Front Based on CV Metrics\n",
    "# =============================================================================\n",
    "results_df = cross_validate_and_grid_search(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    fairness_features_train, fairness_features_test,\n",
    "    param_grid, target_fairness_feature = 'gender_F',\n",
    "    rank_k=250, n_folds=3,\n",
    "    grid_search_results_path=grid_search_results_path,\n",
    "    pareto_frontier_param_path=pareto_frontier_param_path,\n",
    ")\n",
    "\n",
    "def pareto_frontier(results_df):\n",
    "    \"\"\"\n",
    "    Compute the Pareto frontier from the results DataFrame.\n",
    "    :param result_df: DataFrame containing the results with 'cv_rKL' and 'cv_apk' columns.\n",
    "    :return: Boolean mask indicating which points are on the Pareto frontier.\n",
    "    \"\"\"\n",
    "    # Extract costs for Pareto analysis\n",
    "    costs = np.column_stack([-results_df['cv_rKL'].values, results_df['cv_apk'].values])\n",
    "\n",
    "    num_points = costs.shape[0]\n",
    "    is_pareto = np.ones(num_points, dtype=bool)\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            if i != j:\n",
    "                # If point j dominates point i, mark point i as not on the Pareto frontier\n",
    "                if (costs[j] >= costs[i]).all() and (costs[j] > costs[i]).any():\n",
    "                    is_pareto[i] = False\n",
    "                    break\n",
    "    return is_pareto\n",
    "\n",
    "pareto_mask = pareto_frontier(results_df)\n",
    "pareto_df = results_df[pareto_mask]\n",
    "\n",
    "## Saving pareto_params\n",
    "pareto_params = [r['params'] for r in pareto_df.to_dict('records')]\n",
    "with open(pareto_frontier_param_path, 'w') as f:\n",
    "    json.dump(pareto_params, f, indent=4)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Plot the Pareto Front\n",
    "# =============================================================================\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(results_df['cv_apk'], results_df['cv_rKL'], label=\"All Models\", alpha=0.5)\n",
    "plt.scatter(pareto_df['cv_apk'], pareto_df['cv_rKL'], color=\"red\", label=\"Pareto Front\", s=100)\n",
    "plt.xlabel(\"CV APK (Average Precision at K)\")\n",
    "plt.ylabel(\"CV rKL (Rank-aware Normalised KL Divergence)\")\n",
    "plt.title(\"Pareto Front: CV APK vs CV rKL\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Report the Results\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPareto Front Results:\")\n",
    "pareto_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf752681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(X_train, y_train, fairness_train_df,\n",
    "                   protected_feature, rank_k = 250 , n_flods=3):\n",
    "    \n",
    "    def _phi_c(mask_class, top_k_indices):\n",
    "        \"\"\"\n",
    "        Helper function to compute the phi_c value for the target fairness feature.\n",
    "        \"\"\"\n",
    "        k_list = list(range(10, rank_k + 1, 10))\n",
    "        \n",
    "        global_proportion = mask_class.sum() / len(mask_class)  # global prevalence of the protected class\n",
    "\n",
    "        phi_c = 0.0\n",
    "        for k in k_list:\n",
    "            k_indices = top_k_indices[:k]\n",
    "            k_mask = mask_class[k_indices]\n",
    "            k_proportion = k_mask.sum() / k\n",
    "            phi_c += max(0, global_proportion - k_proportion) / np.log2(k)\n",
    "           \n",
    "        \n",
    "        return phi_c\n",
    "    \n",
    "    # def _suggest_params(trial):\n",
    "    #     return dict(\n",
    "    #         n_estimators       = trial.suggest_int(\"n_estimators\",     100, 600),\n",
    "    #         learning_rate      = trial.suggest_float(\"learning_rate\",  1e-3, 0.4, log=True),\n",
    "    #         max_depth          = trial.suggest_int(\"max_depth\",        2, 10),\n",
    "    #         subsample          = trial.suggest_float(\"subsample\",      0.5, 1.0),\n",
    "    #         colsample_bytree   = trial.suggest_float(\"colsample_bytree\",0.5, 1.0),\n",
    "    #         alpha              = trial.suggest_float(\"alpha\",          0.0, 8.0),\n",
    "    #         reg_lambda         = trial.suggest_float(\"reg_lambda\",     0.0, 8.0),\n",
    "    #         scale_pos_weight   = trial.suggest_float(\"scale_pos_weight\",5.0, 25.0),\n",
    "    #         eval_metric        = \"logloss\",\n",
    "    #         random_state       = 42\n",
    "    #     )\n",
    "\n",
    "    def _suggest_params(trial):\n",
    "            return dict(\n",
    "               alpha              = trial.suggest_float(\"alpha\",          0.0, 5.0, step=0.1),\n",
    "               colsample_bytree   = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.05),\n",
    "               learning_rate      = trial.suggest_float(\"learning_rate\",  1e-3, 0.4, log=True),\n",
    "               max_depth          = trial.suggest_int(\"max_depth\",        2, 7, step=1),\n",
    "               n_estimators       = trial.suggest_int(\"n_estimators\",     100, 500, step=50),\n",
    "               reg_lambda         = trial.suggest_float(\"reg_lambda\",     0.0, 5.0, step=0.1),\n",
    "               scale_pos_weight   = trial.suggest_float(\"scale_pos_weight\", 5.0, 20, step=1.0),\n",
    "               subsample          = trial.suggest_float(\"subsample\",      0.5, 1.0, step=0.05),\n",
    "               eval_metric        = \"logloss\",\n",
    "               random_state       = 42\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def objective(trial):\n",
    "        # ————— 1. draw hyper-params ————————————————————————————\n",
    "        params = _suggest_params(trial)      # learning_rate, depth, …\n",
    "        fairness_step      = trial.suggest_float(\"fairness_step\",  0.0, 1.0, step=0.05)\n",
    "        n_iter     = trial.suggest_int(\"n_iter\",   0, 5)\n",
    "\n",
    "        sensitive_categories = fairness_train_df[protected_feature].dropna().unique().tolist()\n",
    "        \n",
    "        if protected_feature not in fairness_train_df.columns:\n",
    "            raise ValueError(f\"Target fairness feature '{protected_feature}' not found in fairness_train_df columns.\")\n",
    "        \n",
    "        if protected_feature == \"gender_F\" and len(sensitive_categories) != 2:\n",
    "            raise ValueError(f\"Expected 2 categories for '{protected_feature}', found {len(sensitive_categories)} categories.\")\n",
    "        if protected_feature != \"gender_F\" and len(sensitive_categories) != 5:\n",
    "            raise ValueError(f\"Expected 5 categories for '{protected_feature}', found {len(sensitive_categories)} categories.\")\n",
    "\n",
    "        # ————— 2. cross-validation loop ————————————————\n",
    "        cv = StratifiedKFold(n_flods, shuffle=True, random_state=42)\n",
    "        fold_rkl, fold_apk = [], []\n",
    "\n",
    "        for tr_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "            X_val, y_val = X_train[val_idx], y_train[val_idx]\n",
    "            A_tr  = fairness_train_df.iloc[tr_idx]\n",
    "            A_val = fairness_train_df.iloc[val_idx]\n",
    "\n",
    "            # —— 2a. adaptive re-weighting ————————————————\n",
    "            w = np.ones(len(y_tr))\n",
    "            for _ in range(n_iter):\n",
    "                tmp = xgb.XGBClassifier(**params,)\n",
    "                tmp.fit(X_tr, y_tr, sample_weight=w)\n",
    "\n",
    "                prob = tmp.predict_proba(X_tr)[:, 1]\n",
    "                topk_idx = np.argsort(prob)[::-1][:rank_k]\n",
    "                # class-wise prevalence shift\n",
    "                for c in sensitive_categories:\n",
    "                    mask_c = (A_tr[protected_feature] == c).to_numpy()\n",
    "                    phi_c = _phi_c(mask_c, topk_idx)\n",
    "                    mask_c_pos = (A_tr[protected_feature] == c) & (y_tr == 1)\n",
    "                    mask_c_pos = mask_c_pos.to_numpy()\n",
    "                    w[mask_c_pos] += fairness_step * phi_c\n",
    "                \n",
    "\n",
    "                 \n",
    "\n",
    "\n",
    "            # —— 2b. final model for this fold ————————————\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_tr, y_tr, sample_weight=w, \n",
    "                    #   eval_set=[(X_val, y_val)],\n",
    "                    #   callbacks=[XGBoostPruningCallback(trial, \"validation_0-logloss\")]\n",
    "                    )\n",
    "\n",
    "            prob_val = model.predict_proba(X_val)[:, 1]\n",
    "            fold_rkl.append(rKL(prob_val, A_val, protected_feature, rank_k))\n",
    "            fold_apk.append(apk_binary(y_val, prob_val, rank_k))\n",
    "\n",
    "        return np.mean(fold_rkl), np.mean(fold_apk)\n",
    "    return objective\n",
    "\n",
    "\n",
    "def load_from_grid_search_results(grid_search_prato_front_json, study):\n",
    "    \"\"\"\n",
    "    Load parameters from grid search results and enqueue top performers.\n",
    "    \"\"\"  \n",
    "    with open(grid_search_prato_front_json, 'r') as file:\n",
    "        params_list = json.load(file)\n",
    "    enqueued_count = 0\n",
    "    for params in params_list:\n",
    "        warm_start_params = {**params,\n",
    "            'random_state': 42,  \n",
    "            'eval_metric': 'logloss',  \n",
    "            'fairness_step': 0.0,  # Start with no fairness adjustment\n",
    "            'n_iter': 0  # Start with no iterations\n",
    "        }\n",
    "        \n",
    "        \n",
    "        study.enqueue_trial(warm_start_params)\n",
    "        enqueued_count += 1\n",
    "    print(f\"Enqueued {enqueued_count} trials from grid search results.\")\n",
    "    return enqueued_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create objective function\n",
    "objective = make_objective(\n",
    "    X_train, y_train, fairness_features_train,\n",
    "    protected_feature=\"gender_F\",\n",
    "    rank_k=250,\n",
    "    n_flods=3\n",
    ")\n",
    "\n",
    "# Basic usage\n",
    "storage = f'sqlite:///my_study.db'\n",
    "\n",
    "# With path variable\n",
    "db_path = './experiment_result/optimization.db'\n",
    "storage = f'sqlite:///{db_path}'\n",
    "\n",
    "# Create study with multi-objective optimization\n",
    "study = optuna.create_study(\n",
    "    storage=storage,\n",
    "    directions=[\"minimize\", \"maximize\"],\n",
    "    sampler=optuna.samplers.TPESampler(multivariate=True, seed=42),\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Enqueue warm start trials\n",
    "print(\"Enqueueing warm start parameters...\")\n",
    "enqueued_count = load_from_grid_search_results(grid_search_prato_front_df, study)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c03ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "print(f\"Starting optimization with {enqueued_count} warm start trials...\")\n",
    "\n",
    "n_trials = 50\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "# Process results\n",
    "all_trials = study.trials\n",
    "trial_data = []\n",
    "for trial in all_trials:\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "        trial_data.append({\n",
    "            'trial_number': trial.number,\n",
    "            'value_rKL': trial.values[0],\n",
    "            'value_apk': trial.values[1],\n",
    "            'params': trial.params,\n",
    "            'is_warm_start': trial.number < enqueued_count,\n",
    "          \n",
    "        })\n",
    "\n",
    "trial_df = pd.DataFrame(trial_data)\n",
    "\n",
    "# Extract Pareto optimal trials\n",
    "pareto_trials = study.best_trials\n",
    "pareto_data = []\n",
    "for trial in pareto_trials:\n",
    "    pareto_data.append({\n",
    "        'trial_number': trial.number,\n",
    "        'value_rKL': trial.values[0],\n",
    "        'value_apk': trial.values[1],\n",
    "        'params': trial.params,\n",
    "        'is_warm_start': trial.number < enqueued_count\n",
    "    })\n",
    "    \n",
    "pareto_df = pd.DataFrame(pareto_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d4905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Enhanced visualization with parameter labels\n",
    "def plot_optimization_results_with_labels(trial_df, pareto_df, key_params_to_show=None):\n",
    "    \"\"\"\n",
    "    Plot optimization results with parameter labels for Pareto points.\n",
    "    \n",
    "    Args:\n",
    "        trial_df: DataFrame with all trials\n",
    "        pareto_df: DataFrame with Pareto optimal trials\n",
    "        key_params_to_show: List of parameter names to show (if None, shows all)\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Plot all trials\n",
    "    ax.scatter(trial_df[~trial_df['is_warm_start']]['value_apk'], \n",
    "               trial_df[~trial_df['is_warm_start']]['value_rKL'], \n",
    "               alpha=0.5, label='Optuna suggested', s=50)\n",
    "    \n",
    "    # Highlight warm start trials\n",
    "    ax.scatter(trial_df[trial_df['is_warm_start']]['value_apk'], \n",
    "               trial_df[trial_df['is_warm_start']]['value_rKL'], \n",
    "               alpha=0.8, s=100, marker='s', label='Warm start')\n",
    "    \n",
    "    # Highlight Pareto front points\n",
    "    pareto_scatter = ax.scatter(pareto_df['value_apk'], pareto_df['value_rKL'], \n",
    "                                color='red', s=150, label='Pareto front', \n",
    "                                edgecolors='black', linewidth=2, zorder=10)\n",
    "    \n",
    "    # Add parameter labels for each Pareto point\n",
    "    for idx, row in pareto_df.iterrows():\n",
    "        params = row['params']\n",
    "        \n",
    "        # Select which parameters to show\n",
    "        if key_params_to_show:\n",
    "            param_text = '\\n'.join([f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\" \n",
    "                                    for k, v in params.items() if k in key_params_to_show])\n",
    "        else:\n",
    "            # Show all parameters (might be crowded)\n",
    "            param_text = '\\n'.join([f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\" \n",
    "                                    for k, v in params.items()])\n",
    "        \n",
    "        # Add text with background box\n",
    "        bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor='wheat', \n",
    "                         edgecolor='black', alpha=0.8)\n",
    "        \n",
    "        # Position text slightly offset from point\n",
    "        ax.annotate(param_text, \n",
    "                    xy=(row['value_apk'], row['value_rKL']),\n",
    "                    xytext=(10, 10), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8,\n",
    "                    bbox=bbox_props,\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "    \n",
    "    ax.set_xlabel('CV APK (Average Precision at K)', fontsize=12)\n",
    "    ax.set_ylabel('CV rKL (gender)', fontsize=12)\n",
    "    ax.set_title('Optuna Multi-objective Optimization with Warm Start and Parameter Labels', fontsize=14)\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
