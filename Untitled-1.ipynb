{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from FeatBoost.feat_selector import FeatureSelector\n",
    "import json\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "import json\n",
    "import csv\n",
    "import uncertainty\n",
    "from scipy.stats import beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4086e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort = pd.read_parquet(\"./DataProcessing/Sep_24_gim_icd10.parquet\")\n",
    "sbk_gim = pd.read_parquet(\"./DataProcessing/Sep_24_sbk_gim_icd10.parquet\")\n",
    "\n",
    "non_gim_cohort = pd.read_parquet(\"./DataProcessing/Sep_24_non_gim_icd10.parquet\")\n",
    "locality = pd.read_csv(\"./fair_interpretable/fair_inter_locality_v2_update.csv\")\n",
    "statcan = pd.read_csv('./fair_interpretable/statcan_table.csv')\n",
    "zero_sum_columns = [col for col in gim_cohort.columns if gim_cohort[col].sum() == 0]\n",
    "gim_cohort = gim_cohort.drop(columns=zero_sum_columns)\n",
    "sbk_gim = sbk_gim.drop(columns=zero_sum_columns)\n",
    "non_gim_cohort = non_gim_cohort.drop(columns=zero_sum_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness = locality.merge(statcan, how = 'left', on = 'da21uid')\n",
    "print(fairness.shape)\n",
    "fairness = fairness[['genc_id', 'households_dwellings_q_DA21','material_resources_q_DA21','age_labourforce_q_DA21','racialized_NC_pop_q_DA21']]\n",
    "fairness.columns = ['genc_id', 'households_dwellings', 'material_resources', 'age_labourforce', 'racialized']\n",
    "del locality\n",
    "del statcan\n",
    "gc.collect()\n",
    "fairness_columns = list(fairness.columns)[1:]\n",
    "print(fairness_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort = gim_cohort.drop_duplicates()\n",
    "gim_cohort = gim_cohort.reset_index(drop=True)\n",
    "gim_cohort = pd.concat([gim_cohort, sbk_gim], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351eb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_rate(y, fairness_features_df, culmulative=False):\n",
    "    \"\"\"\n",
    "    Calculate prevalence rate of delirium across fairness feature groups\n",
    "    \"\"\"\n",
    "    prevalence_rates = []\n",
    "    for fairness_feature in fairness_features_df.columns:\n",
    "        if culmulative:\n",
    "            if fairness_feature == \"gender_F\":\n",
    "                splits = [0]\n",
    "            else:\n",
    "                splits = [1,2,3,4]\n",
    "            for split in splits:\n",
    "                fairness_binary = (fairness_features_df[fairness_feature]>split).astype(int)\n",
    "                group0_mask = fairness_binary == 0\n",
    "                group1_mask = fairness_binary == 1\n",
    "               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dd7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk_binary(y_true, y_pred_probs, k):\n",
    "    \"\"\"\n",
    "    Computes the Average Precision at K (AP@K) for binary predictions.\n",
    "    :param y_true: List or array of ground truth binary labels (0 or 1).\n",
    "    :param y_pred_probs: List or array of predicted probabilities.\n",
    "    :param k: The number of top predictions to consider.\n",
    "    :return: Average Precision at K (AP@K).\n",
    "    \"\"\"\n",
    "    # Sort predictions by predicted probability in descending order\n",
    "    sorted_indices = np.argsort(y_pred_probs)[::-1]\n",
    "    y_true_sorted = np.array(y_true)[sorted_indices]\n",
    "\n",
    "    # Compute precision at each relevant position\n",
    "    num_hits = 0.0\n",
    "    score = 0.0\n",
    "\n",
    "    for i in range(min(k, len(y_true_sorted))):\n",
    "        if y_true_sorted[i] == 1:  # Relevant item\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)  # Precision at position i+1\n",
    "\n",
    "    # Normalize by the number of relevant items or k\n",
    "    return score / min(sum(y_true), k) if sum(y_true) > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def calc_metrics(prediction, prediction_prob, labels, k=10):\n",
    "    acc = accuracy_score(labels, prediction)\n",
    "    f1 = f1_score(labels, prediction, average='binary')  # Use 'micro', 'macro', 'weighted' for multi-class\n",
    "    precision = precision_score(labels, prediction, average='binary')\n",
    "    recall = recall_score(labels, prediction, average='binary')\n",
    "    roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "    # Precision@k calculation\n",
    "    # Sort by prediction probabilities in descending order\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    \n",
    "    # Count true positives in the top k predictions\n",
    "    top_k_labels = np.array(labels)[top_k_indices]\n",
    "    precision_at_k = np.sum(top_k_labels) / k\n",
    "    \n",
    "    return acc, f1, precision, recall, roc_auc, precision_at_k\n",
    "    # return acc, f1, precision, recall, roc_auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_group_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute TPR, FPR, Precision, and Recall for a given group.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else np.nan\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "    recall = TPR  # Recall is the same as TPR\n",
    "    \n",
    "    return TPR, FPR, precision, recall\n",
    "\n",
    "\n",
    "def calc_metrics_at_thresholds(num_features, prediction_prob, labels, fairness_features_df, thresholds=[0.5], k=10, culmulative=False):\n",
    "    \"\"\" \n",
    "    Compute the fairness score at different thresholds at different split, and different fairness features\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Calculate the ROC AUC once, as it does not depend on the threshold\n",
    "    roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "    # Calculate Precision@k\n",
    "    # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    # Create a mask for the top k predictions\n",
    "    top_k_mask = np.zeros(len(labels), dtype=bool)\n",
    "    top_k_mask[top_k_indices] = True\n",
    "\n",
    "    top_k_labels = np.array(labels)[top_k_indices]\n",
    "    precision_at_k = np.sum(top_k_labels) / k\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Apply the threshold to generate predictions\n",
    "        prediction = (prediction_prob >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        correct_prediction = labels == prediction\n",
    "        acc = np.mean(correct_prediction)\n",
    "        f1 = f1_score(labels, prediction, average='binary')\n",
    "        precision = precision_score(labels, prediction, average='binary')\n",
    "        recall = recall_score(labels, prediction, average='binary')\n",
    "        map_k = apk_binary(labels, prediction_prob, k=labels.shape[0])\n",
    "        \n",
    "        # Calculate the percentage of positive predictions\n",
    "        pred_positive_percentage = np.mean(prediction) \n",
    "        true_positive_percentage = np.mean(labels)\n",
    "        \n",
    "        for fairness_feature in list(fairness_features_df.columns):\n",
    "            if culmulative:\n",
    "                ## Culmulative split points for fairness features\n",
    "                if fairness_feature == \"gender_F\":\n",
    "                    splits = [0]\n",
    "                else:\n",
    "                    splits = [1,2,3,4]\n",
    "                col_name_split = \"culmulative_split_point(<= split)\"\n",
    "                \n",
    "            else:\n",
    "                # exact split points for fairness features\n",
    "                splits = fairness_features_df[fairness_feature].unique()\n",
    "                col_name_split = \"exact_split_point(== split)\"\n",
    "            for split in splits:\n",
    "\n",
    "                if culmulative:\n",
    "                    # Convert the fairness feature to binary based culmulative split points ex. (<= split) -> 1 , (> split) -> 0\n",
    "                    fairness_binary = (fairness_features_df[fairness_feature]<=split).astype(int)\n",
    "                else:\n",
    "                    # Convert the fairness feature to binary based exat split points ex.(== split) -> 1, (!= split) -> 0\n",
    "                    fairness_binary = (fairness_features_df[fairness_feature] == split).astype(int)\n",
    "                group0_mask = fairness_binary == 0\n",
    "                group1_mask = fairness_binary == 1\n",
    "\n",
    "                ### prevalence rate\n",
    "                ##“How common is label = 1 in this group in the real world?”\tLarge natural imbalance → model must handle different base-rates.\n",
    "                prevalence_rate1 = (labels[group1_mask].sum() / len(labels[group1_mask])) if len(labels[group1_mask]) > 0 else 0\n",
    "                prevalence_rate0 = (labels[group0_mask].sum() / len(labels[group0_mask])) if len(labels[group0_mask]) > 0 else 0\n",
    "\n",
    "\n",
    "                # calculate prevalence rate of delirium across fairness feature groups @ the top k predictions\n",
    "                #“Among the group’s members who made the shortlist (top-k), what fraction truly need/deserve the action?” (precision of ranking within the group)\t\n",
    "                # One group’s Prev@k ≫ another’s → scarce slots for the second group are ‘wasted’ on false positives or its true positives are being outranked.\n",
    "                prevalence_rate1_k = (labels[group1_mask&top_k_mask].sum() / len(labels[group1_mask&top_k_mask])) if len(labels[group1_mask&top_k_mask]) > 0 else 0\n",
    "                prevalence_rate0_k = (labels[group0_mask&top_k_mask].sum() / len(labels[group0_mask&top_k_mask])) if len(labels[group0_mask&top_k_mask]) > 0 else 0\n",
    "\n",
    "                ### treatment rate\n",
    "                # “With my chosen threshold, how often do I give the positive decision to this group?”\t\n",
    "                # #Gap ≫ prevalence gap → model amplifies imbalance; gap ≪ prevalence gap → model may under-serve high-need group.\n",
    "                treatment_rate1 = (prediction[group1_mask].sum() / len(prediction[group1_mask])) if len(prediction[group1_mask]) > 0 else 0\n",
    "                treatment_rate0 = (prediction[group0_mask].sum() / len(prediction[group0_mask])) if len(prediction[group0_mask]) > 0 else 0\n",
    "\n",
    "                # calculate treatment rate of delirium across fairness feature groups @ the top k predictions\n",
    "                # “What share of the entire group lands in the topk?” (allocation of a limited resource)\t\n",
    "                # TR@k gap shows direct disparate opportunity or burden when capacity is capped \n",
    "                treatment_rate1_k = (group1_mask & top_k_mask).sum() / group1_mask.sum() if group1_mask.sum() > 0 else 0\n",
    "                treatment_rate0_k = (group0_mask & top_k_mask).sum() / group0_mask.sum() if group0_mask.sum() > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "                if sum(group0_mask) == 0 or sum(group1_mask) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                group0_labels = np.array(labels)[group0_mask]\n",
    "                group0_preds = np.array(prediction)[group0_mask]\n",
    "    \n",
    "                group1_labels = np.array(labels)[group1_mask]\n",
    "                group1_preds = np.array(prediction)[group1_mask]\n",
    "\n",
    "                TPR_0, FPR_0, precision_0, recall_0 = compute_group_metrics(group0_labels, group0_preds)\n",
    "                TPR_1, FPR_1, precision_1, recall_1 = compute_group_metrics(group1_labels, group1_preds)\n",
    "\n",
    "                tpr_diff = TPR_1 - TPR_0\n",
    "                fpr_diff = FPR_1 - FPR_0\n",
    "                prec_diff = precision_1 - precision_0\n",
    "                rec_diff = recall_1 - recall_0\n",
    "\n",
    "                # Calculate the tpr, fpr, precision, recall  @k\n",
    "                TPR_0_k, FPR_0_k, precision_0_k, recall_0_k = compute_group_metrics(labels[group0_mask], # y_true  (only group-0 rows)\n",
    "                                                                                    top_k_mask[group0_mask]) # y_pred  (1 if that row is in the top-k)\n",
    "                TPR_1_k, FPR_1_k, precision_1_k, recall_1_k = compute_group_metrics(labels[group1_mask], \n",
    "                                                                                    top_k_mask[group1_mask])\n",
    "                tpr_diff_k = TPR_1_k - TPR_0_k\n",
    "                fpr_diff_k = FPR_1_k - FPR_0_k\n",
    "                prec_diff_k = precision_1_k - precision_0_k\n",
    "                rec_diff_k = recall_1_k - recall_0_k\n",
    "\n",
    "\n",
    "\n",
    "                ##### Bayesian Unfairness and Uncertainty\n",
    "                # Setting correct prediction as the favorable outcome, Bayesian disparity assumes both groups have a 50% chance of receiving the favorable outcome.\n",
    "                E1 = correct_prediction \n",
    "                E2 = True\n",
    "                bayesian_disparity = uncertainty.bayesian_disparity(group0_mask, group1_mask, E1, E2)\n",
    "                bayesian_disparity_abs = np.abs(bayesian_disparity)\n",
    "                uncertainty_value = uncertainty.uncertainty(group0_mask, group1_mask, E1, E2)\n",
    "\n",
    "\n",
    "                # Calculate the uncertainty value@k\n",
    "                E1_k = np.array(labels)[top_k_mask] == prediction[top_k_mask]\n",
    "                E2_k = True\n",
    "                bayesian_disparity_k = uncertainty.bayesian_disparity(group1_mask[top_k_mask], group0_mask[top_k_mask], E1_k, E2_k)\n",
    "                bayesian_disparity_abs_k = np.abs(bayesian_disparity_k)\n",
    "                uncertainty_value_k = uncertainty.uncertainty(group0_mask[top_k_mask], group1_mask[top_k_mask], E1_k, E2_k)\n",
    "\n",
    "                \n",
    "    \n",
    "                results.append({\n",
    "                        'num_features': num_features,\n",
    "                        'threshold': threshold,\n",
    "                        'fairness_feature': fairness_feature,\n",
    "                        col_name_split: split,\n",
    "                        'group_0_size': int((fairness_binary == 0).sum()),\n",
    "                        'group_1_size': int((fairness_binary == 1).sum()),\n",
    "                        'accuracy': acc,\n",
    "                        'f1_score': f1,\n",
    "                        'precision': precision,                        \n",
    "                        'recall': recall,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'precision_at_k': precision_at_k,\n",
    "                        'map@k': map_k,\n",
    "                        'pred_positive_percentage': pred_positive_percentage,\n",
    "                        'true_positive_percentage': true_positive_percentage,\n",
    "                        'tpr_diff_abs': abs(tpr_diff),\n",
    "                        'fpr_diff_abs': abs(fpr_diff),\n",
    "                        'tpr_diff_raw': tpr_diff,\n",
    "                        'fpr_diff_raw': fpr_diff,\n",
    "                        'precision_diff_abs': abs(prec_diff),\n",
    "                        'recall_diff_abs': abs(rec_diff),\n",
    "                        'equalized_odds_max': max(abs(tpr_diff), abs(fpr_diff)),\n",
    "                        'equalized_odds': 0.5*(abs(tpr_diff) + abs(fpr_diff)),\n",
    "                        'bayesian_disparity': bayesian_disparity,\n",
    "                        'bayesian_disparity_abs':bayesian_disparity_abs,\n",
    "                        'bayesian_uncertainty': uncertainty_value,\n",
    "                        'prevalence_rate1': prevalence_rate1,\n",
    "                        'prevalence_rate0': prevalence_rate0,\n",
    "                        'treatment_rate1': treatment_rate1,\n",
    "                        'treatment_rate0': treatment_rate0,\n",
    "                        'prevalence_rate1@k': prevalence_rate1_k,\n",
    "                        'prevalence_rate0@k': prevalence_rate0_k,\n",
    "                        'treatment_rate1@k': treatment_rate1_k,\n",
    "                        'treatment_rate0@k': treatment_rate0_k,\n",
    "                        'tpr_0@k': TPR_0_k,\n",
    "                        'fpr_0@k': FPR_0_k,\n",
    "                        'precision_0@k': precision_0_k,\n",
    "                        'recall_0@k': recall_0_k,\n",
    "                        'tpr_1@k': TPR_1_k,\n",
    "                        'fpr_1@k': FPR_1_k,\n",
    "                        'precision_1@k': precision_1_k,\n",
    "                        'recall_1@k': recall_1_k,\n",
    "                        'tpr_diff@k': tpr_diff_k,\n",
    "                        'fpr_diff@k': fpr_diff_k,\n",
    "                        'precision_diff@k': prec_diff_k,\n",
    "                        'recall_diff@k': rec_diff_k,\n",
    "                        'bayesian_disparity@k': bayesian_disparity_k,\n",
    "                        'bayesian_disparity_abs@k': bayesian_disparity_abs_k,\n",
    "                        'bayesian_uncertainty@k': uncertainty_value_k,\n",
    "                        'equalized_odds_max@k': max(abs(tpr_diff_k), abs(fpr_diff_k)),\n",
    "                        'equalized_odds@k': 0.5*(abs(tpr_diff_k) + abs(fpr_diff_k))\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    # max_eod_df = df.loc[df.groupby('fairness_feature')['equalized_odds'].idxmax()]\n",
    "            \n",
    "            \n",
    "                    \n",
    "\n",
    " \n",
    "            \n",
    "    return df #max_eod_df\n",
    "\n",
    "\n",
    "def bootstrap_sample(data, proportion=0.8):\n",
    "    \"\"\"\n",
    "    Bootstrap sampling function.\n",
    "    :param data: DataFrame to sample from.\n",
    "    :param proportion: Proportion of the data to sample.\n",
    "    :return: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    n_samples = int(len(data) * proportion)\n",
    "    return data.sample(n_samples, replace=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dist_plot_top_k(prediction_prob, fairness_feature, k=10):\n",
    "    # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_fairness_feature = np.array(fairness_feature)[top_k_indices]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the distribution of the top k predictions\n",
    "    ax[0].hist(prediction_prob[top_k_indices], bins=20, color='skyblue', edgecolor='black')\n",
    "    ax[0].set_title(f'Distribution of Top {k} Predictions')\n",
    "    ax[0].set_xlabel('Prediction Probability')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    \n",
    "    # Plot the distribution of the fairness feature in the top k predictions\n",
    "    ax[1].hist(top_k_fairness_feature, bins=20, color='lightcoral', edgecolor='black')\n",
    "    ax[1].set_title(f'Distribution of Fairness Feature in Top {k} Predictions')\n",
    "    ax[1].set_xlabel('Fairness Feature Value')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_sample(data, proportion=0.8):\n",
    "    \"\"\"\n",
    "    Bootstrap sampling function.\n",
    "    :param data: DataFrame to sample from.\n",
    "    :param proportion: Proportion of the data to sample.\n",
    "    :return: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    n_samples = int(len(data) * proportion)\n",
    "    return data.sample(n_samples, replace=True, random_state=42)\n",
    "\n",
    "def bootstrap_confidence_interval(data, metric_func, n_iterations=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a metric using bootstrap sampling.\n",
    "    :param data: DataFrame containing the data.\n",
    "    :param metric_func: Function to calculate the metric.\n",
    "    :param n_iterations: Number of bootstrap iterations.\n",
    "    :param alpha: Significance level for the confidence interval.\n",
    "    :return: Tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "    for i in range(n_iterations):\n",
    "        sample = bootstrap_sample(data)\n",
    "        metric_df  = metric_func(sample)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bootstrap_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e5a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda4c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor = torch.tensor([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]])\n",
    "torch.ones(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00be5afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1090, -0.7893,  1.3153,  0.9808],\n",
       "        [-0.2056,  0.2684, -0.0027,  1.8391],\n",
       "        [ 1.4302,  0.5655,  0.7846,  0.1274]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3932e5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(a_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b65f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2,3,4)\n",
    "z = x.reshape(6,4)                       # merges first two dims\n",
    "z\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac6c052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]],\n",
       "\n",
       "        [[12, 16, 20],\n",
       "         [13, 17, 21],\n",
       "         [14, 18, 22],\n",
       "         [15, 19, 23]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (N, C, H, W)  →  (N, H*W, C)\n",
    "out = x.permute(0,2,1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e54a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, num_features):\n",
    "    self.num_features = num_features\n",
    "    self.model = LogisticRegression(max_iter=1000)\n",
    "    self.feature_selector = FeatureSelector()\n",
    "    self.feature_selector.set_params(num_features=num_features)\n",
    "    self.feature_selector.set_params(model=self.model)\n",
    "\n",
    "def __iter__(self):\n",
    "    return self\n",
    "def __next__(self):\n",
    "    if self.current_index >= len(self.data):\n",
    "        raise StopIteration\n",
    "    batch = self.data[self.current_index:self.current_index + self.batch_size]\n",
    "    self.current_index += self.batch_size\n",
    "    return batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
