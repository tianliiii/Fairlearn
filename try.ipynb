{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from FeatBoost.feat_selector import FeatureSelector\n",
    "import json\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "import json\n",
    "import csv\n",
    "import uncertainty\n",
    "from scipy.stats import beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4086e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort = pd.read_parquet(\"./DataProcessing/Sep_24_gim_icd10.parquet\")\n",
    "sbk_gim = pd.read_parquet(\"./DataProcessing/Sep_24_sbk_gim_icd10.parquet\")\n",
    "\n",
    "non_gim_cohort = pd.read_parquet(\"./DataProcessing/Sep_24_non_gim_icd10.parquet\")\n",
    "locality = pd.read_csv(\"./fair_interpretable/fair_inter_locality_v2_update.csv\")\n",
    "statcan = pd.read_csv('./fair_interpretable/statcan_table.csv')\n",
    "zero_sum_columns = [col for col in gim_cohort.columns if gim_cohort[col].sum() == 0]\n",
    "gim_cohort = gim_cohort.drop(columns=zero_sum_columns)\n",
    "sbk_gim = sbk_gim.drop(columns=zero_sum_columns)\n",
    "non_gim_cohort = non_gim_cohort.drop(columns=zero_sum_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness = locality.merge(statcan, how = 'left', on = 'da21uid')\n",
    "print(fairness.shape)\n",
    "fairness = fairness[['genc_id', 'households_dwellings_q_DA21','material_resources_q_DA21','age_labourforce_q_DA21','racialized_NC_pop_q_DA21']]\n",
    "fairness.columns = ['genc_id', 'households_dwellings', 'material_resources', 'age_labourforce', 'racialized']\n",
    "del locality\n",
    "del statcan\n",
    "gc.collect()\n",
    "fairness_columns = list(fairness.columns)[1:]\n",
    "print(fairness_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort = gim_cohort.drop_duplicates()\n",
    "gim_cohort = gim_cohort.reset_index(drop=True)\n",
    "gim_cohort = pd.concat([gim_cohort, sbk_gim], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351eb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "gim_cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_rate(y, fairness_features_df, culmulative=False):\n",
    "    \"\"\"\n",
    "    Calculate prevalence rate of delirium across fairness feature groups\n",
    "    \"\"\"\n",
    "    prevalence_rates = []\n",
    "    for fairness_feature in fairness_features_df.columns:\n",
    "        if culmulative:\n",
    "            if fairness_feature == \"gender_F\":\n",
    "                splits = [0]\n",
    "            else:\n",
    "                splits = [1,2,3,4]\n",
    "            for split in splits:\n",
    "                fairness_binary = (fairness_features_df[fairness_feature]>split).astype(int)\n",
    "                group0_mask = fairness_binary == 0\n",
    "                group1_mask = fairness_binary == 1\n",
    "               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_normal(model, X_test):\n",
    "    preds = model.predict(X_test)\n",
    "    pred_probs = model.predict_proba(X_test)[:, 1]\n",
    "    pred_probs_all = model.predict_proba(X_test)\n",
    "    return preds, pred_probs, pred_probs_all\n",
    "\n",
    "def calc_metrics(prediction, prediction_prob, labels, k=10):\n",
    "    acc = accuracy_score(labels, prediction)\n",
    "    f1 = f1_score(labels, prediction, average='binary')  # Use 'micro', 'macro', 'weighted' for multi-class\n",
    "    precision = precision_score(labels, prediction, average='binary')\n",
    "    recall = recall_score(labels, prediction, average='binary')\n",
    "    roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "    # Precision@k calculation\n",
    "    # Sort by prediction probabilities in descending order\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    \n",
    "    # Count true positives in the top k predictions\n",
    "    top_k_labels = np.array(labels)[top_k_indices]\n",
    "    precision_at_k = np.sum(top_k_labels) / k\n",
    "    \n",
    "    return acc, f1, precision, recall, roc_auc, precision_at_k\n",
    "    # return acc, f1, precision, recall, roc_auc\n",
    "\n",
    "\n",
    "def expected_calibration_error(samples, true_labels, M=10):\n",
    "    # uniform binning approach with M number of bins\n",
    "    bin_boundaries = np.linspace(0, 1, M + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    # get max probability per sample i\n",
    "    confidences = np.max(samples, axis=1)\n",
    "    # get predictions from confidences (positional in this case)\n",
    "    predicted_label = np.argmax(samples, axis=1)\n",
    "\n",
    "    # get a boolean list of correct/false predictions\n",
    "    accuracies = predicted_label==true_labels\n",
    "\n",
    "    ece = np.zeros(1)\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # determine if sample is in bin m (between bin lower & upper)\n",
    "        in_bin = np.logical_and(confidences > bin_lower.item(), confidences <= bin_upper.item())\n",
    "        # can calculate the empirical probability of a sample falling into bin m: (|Bm|/n)\n",
    "        prob_in_bin = in_bin.mean()\n",
    "\n",
    "        if prob_in_bin.item() > 0:\n",
    "            # get the accuracy of bin m: acc(Bm)\n",
    "            accuracy_in_bin = accuracies[in_bin].mean()\n",
    "            # get the average confidence of bin m: conf(Bm)\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            # calculate |acc(Bm) - conf(Bm)| * (|Bm|/n) for bin m and add to the total ECE\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prob_in_bin\n",
    "    return ece[0]\n",
    "\n",
    "def ace(samples, true_labels, bin_size=25):\n",
    "    \"\"\"\n",
    "    Calculate the Adaptive Calibration Error (ACE) for a set of predictions.\n",
    "    \n",
    "    Args:\n",
    "    samples (np.ndarray): Array of predicted probabilities for each class (shape: [num_samples, num_classes]).\n",
    "    true_labels (np.ndarray): Array of true class labels.\n",
    "    bin_size (int): Number of points in each bin.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated Adaptive Calibration Error (ACE).\n",
    "    \"\"\"\n",
    "    num_samples = samples.shape[0]\n",
    "    assert num_samples == len(true_labels), \"Number of samples and true labels must match.\"\n",
    "\n",
    "    # Get max probability per sample\n",
    "    confidences = np.max(samples, axis=1)\n",
    "    # confidences = samples[:, 1]\n",
    "\n",
    "    # Get predictions from confidences\n",
    "    predicted_label = np.argmax(samples, axis=1)\n",
    "\n",
    "    # Get a boolean list of correct/false predictions\n",
    "    accuracies = predicted_label == true_labels\n",
    "\n",
    "    # Sort by confidence\n",
    "    sorted_indices = np.argsort(confidences)\n",
    "    sorted_confidences = confidences[sorted_indices]\n",
    "    sorted_accuracies = accuracies[sorted_indices]\n",
    "\n",
    "    ace = 0.0\n",
    "    start_idx = 0\n",
    "    \n",
    "    mean_conf = []\n",
    "    mean_acc = []\n",
    "    while start_idx < num_samples:\n",
    "        end_idx = min(start_idx + bin_size, num_samples)\n",
    "        \n",
    "        bin_confidences = sorted_confidences[start_idx:end_idx]\n",
    "        bin_accuracies = sorted_accuracies[start_idx:end_idx]\n",
    "\n",
    "        # Mean predicted probability in the bin\n",
    "        mean_confidence = np.mean(bin_confidences)\n",
    "        # Mean accuracy in the bin\n",
    "        mean_accuracy = np.mean(bin_accuracies)\n",
    "        \n",
    "        mean_conf.append(mean_confidence)\n",
    "        mean_acc.append(mean_accuracy)\n",
    "\n",
    "        # Calculate bin contribution to ACE\n",
    "        ace += np.abs(mean_confidence - mean_accuracy) * len(bin_confidences) / num_samples\n",
    "\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return ace, mean_conf, mean_acc\n",
    "\n",
    "# def calc_metrics_at_thresholds(num_features, prediction_prob, labels, thresholds=[0.5], k=10):\n",
    "#     results = []\n",
    "    \n",
    "#     # Calculate the ROC AUC once, as it does not depend on the threshold\n",
    "#     roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "#     # Calculate Precision@k\n",
    "#     # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "#     sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "#     top_k_indices = sorted_indices[:k]\n",
    "#     top_k_labels = np.array(labels)[top_k_indices]\n",
    "#     precision_at_k = np.sum(top_k_labels) / k\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         # Apply the threshold to generate predictions\n",
    "#         prediction = (prediction_prob >= threshold).astype(int)\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         acc = accuracy_score(labels, prediction)\n",
    "#         f1 = f1_score(labels, prediction, average='binary')\n",
    "#         precision = precision_score(labels, prediction, average='binary')\n",
    "#         recall = recall_score(labels, prediction, average='binary')\n",
    "#         map_k = apk_binary(labels, prediction_prob, k=labels.shape[0])\n",
    "        \n",
    "#         # Calculate the percentage of positive predictions\n",
    "#         pred_positive_percentage = np.mean(prediction) \n",
    "#         true_positive_percentage = np.mean(labels)\n",
    "        \n",
    "#         # Append the results as a row to the list\n",
    "#         results.append({\n",
    "#             'num_features': num_features,\n",
    "#             'threshold': threshold,\n",
    "#             'accuracy': acc,\n",
    "#             'f1_score': f1,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'roc_auc': roc_auc,\n",
    "#             'precision_at_k': precision_at_k,\n",
    "#             'map@k': map_k,\n",
    "#             'pred_positive_percentage': pred_positive_percentage,\n",
    "#             'true_positive_percentage': true_positive_percentage\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# helper function of calc_metrics_at_thresholds\n",
    "def compute_group_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute TPR, FPR, Precision, and Recall for a given group.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else np.nan\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "    recall = TPR  # Recall is the same as TPR\n",
    "    \n",
    "    return TPR, FPR, precision, recall\n",
    "\n",
    "# def calc_metrics_at_thresholds(num_features, prediction_prob, labels, fairness_features_df, thresholds=[0.5], k=10, cumulative=False):\n",
    "#     \"\"\" \n",
    "#     Compute the fairness score at different thresholds at different split, and different fairness features\n",
    "#     \"\"\"\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     # Calculate the ROC AUC once, as it does not depend on the threshold\n",
    "#     roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "#     # Calculate Precision@k\n",
    "#     # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "#     sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "#     top_k_indices = sorted_indices[:k]\n",
    "#     top_k_labels = np.array(labels)[top_k_indices]\n",
    "#     precision_at_k = np.sum(top_k_labels) / k\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         # Apply the threshold to generate predictions\n",
    "#         prediction = (prediction_prob >= threshold).astype(int)\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         correct_prediction = labels == prediction\n",
    "#         acc = np.mean(correct_prediction)\n",
    "#         f1 = f1_score(labels, prediction, average='binary')\n",
    "#         precision = precision_score(labels, prediction, average='binary')\n",
    "#         recall = recall_score(labels, prediction, average='binary')\n",
    "#         map_k = apk_binary(labels, prediction_prob, k=labels.shape[0])\n",
    "        \n",
    "#         # Calculate the percentage of positive predictions\n",
    "#         pred_positive_percentage = np.mean(prediction) \n",
    "#         true_positive_percentage = np.mean(labels)\n",
    "        \n",
    "#         for fairness_feature in list(fairness_features_df.columns):\n",
    "#             if fairness_feature == \"gender_F\":\n",
    "#                 splits = [0]\n",
    "#             else:\n",
    "#                 splits = [1,2,3,4]\n",
    "#             for split in splits:\n",
    "\n",
    "#                 # Split the data into privileged and unprivileged groups based on the fairness feature\n",
    "    \n",
    "#                 fairness_binary = (fairness_features_df[fairness_feature]>split).astype(int)\n",
    "#                 group0_mask = fairness_binary == 0\n",
    "#                 group1_mask = fairness_binary == 1\n",
    "               \n",
    "#                 if sum(group0_mask) == 0 or sum(group1_mask) == 0:\n",
    "#                     continue\n",
    "                    \n",
    "#                 group0_labels = np.array(labels)[group0_mask]\n",
    "#                 group0_preds = np.array(prediction)[group0_mask]\n",
    "    \n",
    "#                 group1_labels = np.array(labels)[group1_mask]\n",
    "#                 group1_preds = np.array(prediction)[group1_mask]\n",
    "\n",
    "#                 TPR_0, FPR_0, precision_0, recall_0 = compute_group_metrics(group0_labels, group0_preds)\n",
    "#                 TPR_1, FPR_1, precision_1, recall_1 = compute_group_metrics(group1_labels, group1_preds)\n",
    "\n",
    "#                 tpr_diff = TPR_1 - TPR_0\n",
    "#                 fpr_diff = FPR_1 - FPR_0\n",
    "#                 prec_diff = precision_1 - precision_0\n",
    "#                 rec_diff = recall_1 - recall_0\n",
    "\n",
    "#                 ##### Bayesian Unfairness and Uncertainty\n",
    "#                 # Setting correct prediction as the favorable outcome, Bayesian disparity assumes both groups have a 50% chance of receiving the favorable outcome.\n",
    "#                 E1 = correct_prediction \n",
    "#                 E2 = True\n",
    "#                 bayesian_disparity = uncertainty.bayesian_disparity(group0_mask, group1_mask, E1, E2)\n",
    "#                 bayesian_disparity_abs = np.abs(bayesian_disparity)\n",
    "#                 uncertainty_value = uncertainty.uncertainty(group0_mask, group1_mask, E1, E2)\n",
    "\n",
    "                \n",
    "    \n",
    "#                 results.append({\n",
    "#                         'split_point': split,\n",
    "#                         'num_features': num_features,\n",
    "#                         'threshold': threshold,\n",
    "#                         'fairness_feature': fairness_feature,\n",
    "#                         'group_0_size': int((fairness_binary == 0).sum()),\n",
    "#                         'group_1_size': int((fairness_binary == 1).sum()),\n",
    "#                         'accuracy': acc,\n",
    "#                         'f1_score': f1,\n",
    "#                         'precision': precision,                        \n",
    "#                         'recall': recall,\n",
    "#                         'roc_auc': roc_auc,\n",
    "#                         'precision_at_k': precision_at_k,\n",
    "#                         'map@k': map_k,\n",
    "#                         'pred_positive_percentage': pred_positive_percentage,\n",
    "#                         'true_positive_percentage': true_positive_percentage,\n",
    "#                         'tpr_diff_abs': abs(tpr_diff),\n",
    "#                         'fpr_diff_abs': abs(fpr_diff),\n",
    "#                         'tpr_diff_raw': tpr_diff,\n",
    "#                         'fpr_diff_raw': fpr_diff,\n",
    "#                         'precision_diff_abs': abs(prec_diff),\n",
    "#                         'recall_diff_abs': abs(rec_diff),\n",
    "#                         'equalized_odds_max': max(abs(tpr_diff), abs(fpr_diff)),\n",
    "#                         'equalized_odds': 0.5*(abs(tpr_diff) + abs(fpr_diff)),\n",
    "#                         'bayesian_disparity': bayesian_disparity,\n",
    "#                         'bayesian_disparity_abs':bayesian_disparity_abs,\n",
    "#                         'bayesian_uncertainty': uncertainty_value\n",
    "#                         })\n",
    "#                 df = pd.DataFrame(results)\n",
    "#                 # max_eod_df = df.loc[df.groupby('fairness_feature')['equalized_odds'].idxmax()]\n",
    " \n",
    "            \n",
    "#     return df #max_eod_df\n",
    "def calc_metrics_at_thresholds(num_features, prediction_prob, labels, fairness_features_df, thresholds=[0.5], k=10, cumulative=False):\n",
    "    \"\"\" \n",
    "    Compute the fairness score at different thresholds at different split, and different fairness features\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Calculate the ROC AUC once, as it does not depend on the threshold\n",
    "    roc_auc = roc_auc_score(labels, prediction_prob)\n",
    "    \n",
    "    # Calculate Precision@k\n",
    "    # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    # Create a mask for the top k predictions\n",
    "    top_k_mask = np.zeros(len(labels), dtype=bool)\n",
    "    top_k_mask[top_k_indices] = True\n",
    "\n",
    "    top_k_labels = np.array(labels)[top_k_indices]\n",
    "    precision_at_k = np.sum(top_k_labels) / k\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Apply the threshold to generate predictions\n",
    "        prediction = (prediction_prob >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        correct_prediction = labels == prediction\n",
    "        acc = np.mean(correct_prediction)\n",
    "        f1 = f1_score(labels, prediction, average='binary')\n",
    "        precision = precision_score(labels, prediction, average='binary')\n",
    "        recall = recall_score(labels, prediction, average='binary')\n",
    "        map_k = apk_binary(labels, prediction_prob, k=labels.shape[0])\n",
    "        \n",
    "        # Calculate the percentage of positive predictions\n",
    "        pred_positive_percentage = np.mean(prediction) \n",
    "        true_positive_percentage = np.mean(labels)\n",
    "        \n",
    "        for fairness_feature in list(fairness_features_df.columns):\n",
    "            if cumulative:\n",
    "                ## Cumulative split points for fairness features\n",
    "                if fairness_feature == \"gender_F\":\n",
    "                    splits = [0]\n",
    "                else:\n",
    "                    splits = [1,2,3,4]\n",
    "                col_name_split = \"cumulative_split_point(<= split)\"\n",
    "                \n",
    "            else:\n",
    "                # exact split points for fairness features\n",
    "                splits = sorted(fairness_features_df[fairness_feature].unique())\n",
    "                col_name_split = \"exact_split_point(== split)\"\n",
    "            for split in splits:\n",
    "\n",
    "                if cumulative:\n",
    "                    # Convert the fairness feature to binary based cumulative split points ex. (<= split) -> 1 , (> split) -> 0\n",
    "                    fairness_binary = (fairness_features_df[fairness_feature]<=split).astype(int)\n",
    "                else:\n",
    "                    # Convert the fairness feature to binary based exat split points ex.(== split) -> 1, (!= split) -> 0\n",
    "                    fairness_binary = (fairness_features_df[fairness_feature] == split).astype(int)\n",
    "                group0_mask = fairness_binary == 0\n",
    "                group1_mask = fairness_binary == 1\n",
    "\n",
    "                ### prevalence rate\n",
    "                ##“How common is label = 1 in this group in the real world?”\tLarge natural imbalance → model must handle different base-rates.\n",
    "                prevalence_rate1 = (labels[group1_mask].sum() / len(labels[group1_mask])) if len(labels[group1_mask]) > 0 else 0\n",
    "                prevalence_rate0 = (labels[group0_mask].sum() / len(labels[group0_mask])) if len(labels[group0_mask]) > 0 else 0\n",
    "\n",
    "\n",
    "                # calculate prevalence rate of delirium across fairness feature groups @ the top k predictions\n",
    "                #“Among the group’s members who made the shortlist (top-k), what fraction truly need/deserve the action?” (precision of ranking within the group)\t\n",
    "                # One group’s Prev@k ≫ another’s → scarce slots for the second group are ‘wasted’ on false positives or its true positives are being outranked.\n",
    "                prevalence_rate1_k = (labels[group1_mask&top_k_mask].sum() / len(labels[group1_mask&top_k_mask])) if len(labels[group1_mask&top_k_mask]) > 0 else 0\n",
    "                prevalence_rate0_k = (labels[group0_mask&top_k_mask].sum() / len(labels[group0_mask&top_k_mask])) if len(labels[group0_mask&top_k_mask]) > 0 else 0\n",
    "\n",
    "                ### treatment rate\n",
    "                # “With my chosen threshold, how often do I give the positive decision to this group?”\t\n",
    "                # #Gap ≫ prevalence gap → model amplifies imbalance; gap ≪ prevalence gap → model may under-serve high-need group.\n",
    "                treatment_rate1 = (prediction[group1_mask].sum() / len(prediction[group1_mask])) if len(prediction[group1_mask]) > 0 else 0\n",
    "                treatment_rate0 = (prediction[group0_mask].sum() / len(prediction[group0_mask])) if len(prediction[group0_mask]) > 0 else 0\n",
    "\n",
    "                # calculate treatment rate of delirium across fairness feature groups @ the top k predictions\n",
    "                # “What share of the entire group lands in the topk?” (allocation of a limited resource)\t\n",
    "                # TR@k gap shows direct disparate opportunity or burden when capacity is capped \n",
    "                treatment_rate1_k = (group1_mask & top_k_mask).sum() / group1_mask.sum() if group1_mask.sum() > 0 else 0\n",
    "                treatment_rate0_k = (group0_mask & top_k_mask).sum() / group0_mask.sum() if group0_mask.sum() > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "                if sum(group0_mask) == 0 or sum(group1_mask) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                group0_labels = np.array(labels)[group0_mask]\n",
    "                group0_preds = np.array(prediction)[group0_mask]\n",
    "    \n",
    "                group1_labels = np.array(labels)[group1_mask]\n",
    "                group1_preds = np.array(prediction)[group1_mask]\n",
    "\n",
    "                TPR_0, FPR_0, precision_0, recall_0 = compute_group_metrics(group0_labels, group0_preds)\n",
    "                TPR_1, FPR_1, precision_1, recall_1 = compute_group_metrics(group1_labels, group1_preds)\n",
    "\n",
    "                tpr_diff = TPR_1 - TPR_0\n",
    "                fpr_diff = FPR_1 - FPR_0\n",
    "                prec_diff = precision_1 - precision_0\n",
    "                rec_diff = recall_1 - recall_0\n",
    "\n",
    "                # Calculate the tpr, fpr, precision, recall  @k\n",
    "                TPR_0_k, FPR_0_k, precision_0_k, recall_0_k = compute_group_metrics(labels[group0_mask], # y_true  (only group-0 rows)\n",
    "                                                                                    top_k_mask[group0_mask]) # y_pred  (1 if that row is in the top-k)\n",
    "                TPR_1_k, FPR_1_k, precision_1_k, recall_1_k = compute_group_metrics(labels[group1_mask], \n",
    "                                                                                    top_k_mask[group1_mask])\n",
    "                tpr_diff_k = TPR_1_k - TPR_0_k\n",
    "                fpr_diff_k = FPR_1_k - FPR_0_k\n",
    "                prec_diff_k = precision_1_k - precision_0_k\n",
    "                rec_diff_k = recall_1_k - recall_0_k\n",
    "\n",
    "\n",
    "\n",
    "                ##### Bayesian Unfairness and Uncertainty\n",
    "                # Setting correct prediction as the favorable outcome, Bayesian disparity assumes both groups have a 50% chance of receiving the favorable outcome.\n",
    "                E1 = correct_prediction \n",
    "                E2 = True\n",
    "                bayesian_disparity = uncertainty.bayesian_disparity(group0_mask, group1_mask, E1, E2)\n",
    "                bayesian_disparity_abs = np.abs(bayesian_disparity)\n",
    "                uncertainty_value = uncertainty.uncertainty(group0_mask, group1_mask, E1, E2)\n",
    "\n",
    "\n",
    "                # Calculate the uncertainty value@k\n",
    "                E1_k = labels&top_k_mask&prediction\n",
    "                E2_k = True\n",
    "                bayesian_disparity_k = uncertainty.bayesian_disparity(group1_mask, group0_mask, E1_k, E2_k)\n",
    "                bayesian_disparity_abs_k = np.abs(bayesian_disparity_k)\n",
    "                uncertainty_value_k = uncertainty.uncertainty(group0_mask, group1_mask, E1_k, E2_k)\n",
    "\n",
    "                \n",
    "    \n",
    "                results.append({\n",
    "                        'num_features': num_features,\n",
    "                        'threshold': threshold,\n",
    "                        'fairness_feature': fairness_feature,\n",
    "                        col_name_split: split,\n",
    "                        'k': k,\n",
    "                        'group_0_size': int((fairness_binary == 0).sum()),\n",
    "                        'group_1_size': int((fairness_binary == 1).sum()),\n",
    "                        'accuracy': acc,\n",
    "                        'f1_score': f1,\n",
    "                        'precision': precision,                        \n",
    "                        'recall': recall,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'precision_at_k': precision_at_k,\n",
    "                        'map@k': map_k,\n",
    "                        'pred_positive_percentage': pred_positive_percentage,\n",
    "                        'true_positive_percentage': true_positive_percentage,\n",
    "                        'tpr_diff_abs': abs(tpr_diff),\n",
    "                        'fpr_diff_abs': abs(fpr_diff),\n",
    "                        'tpr_diff_raw': tpr_diff,\n",
    "                        'fpr_diff_raw': fpr_diff,\n",
    "                        'precision_diff_abs': abs(prec_diff),\n",
    "                        'recall_diff_abs': abs(rec_diff),\n",
    "                        'equalized_odds_max': max(abs(tpr_diff), abs(fpr_diff)),\n",
    "                        'equalized_odds': 0.5*(abs(tpr_diff) + abs(fpr_diff)),\n",
    "                        'bayesian_disparity': bayesian_disparity,\n",
    "                        'bayesian_disparity_abs':bayesian_disparity_abs,\n",
    "                        'bayesian_uncertainty': uncertainty_value,\n",
    "                        'prevalence_rate1': prevalence_rate1,\n",
    "                        'prevalence_rate0': prevalence_rate0,\n",
    "                        'treatment_rate1': treatment_rate1,\n",
    "                        'treatment_rate0': treatment_rate0,\n",
    "                        'prevalence_rate1@k': prevalence_rate1_k,\n",
    "                        'prevalence_rate0@k': prevalence_rate0_k,\n",
    "                        'treatment_rate1@k': treatment_rate1_k,\n",
    "                        'treatment_rate0@k': treatment_rate0_k,\n",
    "                        'tpr_0@k': TPR_0_k,\n",
    "                        'fpr_0@k': FPR_0_k,\n",
    "                        'precision_0@k': precision_0_k,\n",
    "                        'recall_0@k': recall_0_k,\n",
    "                        'tpr_1@k': TPR_1_k,\n",
    "                        'fpr_1@k': FPR_1_k,\n",
    "                        'precision_1@k': precision_1_k,\n",
    "                        'recall_1@k': recall_1_k,\n",
    "                        'tpr_diff@k': tpr_diff_k,\n",
    "                        'fpr_diff@k': fpr_diff_k,\n",
    "                        'precision_diff@k': prec_diff_k,\n",
    "                        'recall_diff@k': rec_diff_k,\n",
    "                        'bayesian_disparity@k': bayesian_disparity_k,\n",
    "                        'bayesian_disparity_abs@k': bayesian_disparity_abs_k,\n",
    "                        'bayesian_uncertainty@k': uncertainty_value_k,\n",
    "                        'equalized_odds_max@k': max(abs(tpr_diff_k), abs(fpr_diff_k)),\n",
    "                        'equalized_odds@k': 0.5*(abs(tpr_diff_k) + abs(fpr_diff_k))\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    # max_eod_df = df.loc[df.groupby('fairness_feature')['equalized_odds'].idxmax()]\n",
    "            \n",
    "            \n",
    "                    \n",
    "\n",
    " \n",
    "            \n",
    "    return df #max_eod_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dist_plot_top_k(prediction_prob, fairness_feature, k=10):\n",
    "    # Sort by prediction probabilities in descending order and take the top k predictions\n",
    "    sorted_indices = np.argsort(prediction_prob)[::-1]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_fairness_feature = np.array(fairness_feature)[top_k_indices]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the distribution of the top k predictions\n",
    "    ax[0].hist(prediction_prob[top_k_indices], bins=20, color='skyblue', edgecolor='black')\n",
    "    ax[0].set_title(f'Distribution of Top {k} Predictions')\n",
    "    ax[0].set_xlabel('Prediction Probability')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    \n",
    "    # Plot the distribution of the fairness feature in the top k predictions\n",
    "    ax[1].hist(top_k_fairness_feature, bins=20, color='lightcoral', edgecolor='black')\n",
    "    ax[1].set_title(f'Distribution of Fairness Feature in Top {k} Predictions')\n",
    "    ax[1].set_xlabel('Fairness Feature Value')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _fairness_metrics_one_sample(num_features,\n",
    "                                 prediction_prob,\n",
    "                                 labels,\n",
    "                                 fairness_features_df,\n",
    "                                 **calc_kwargs):\n",
    "    \"\"\"\n",
    "    Returns the DataFrame produced by `calc_metrics_at_thresholds`, but *only*\n",
    "    the columns that contain '@k'.\n",
    "    \"\"\"\n",
    "    df_metrics = calc_metrics_at_thresholds(\n",
    "        num_features=num_features,\n",
    "        prediction_prob=prediction_prob,\n",
    "        labels=labels,\n",
    "        fairness_features_df=fairness_features_df,\n",
    "        **calc_kwargs                      # thresholds, k, cumulative…\n",
    "    )\n",
    "    at_k_cols = [c for c in df_metrics.columns if '@k' in c]\n",
    "    key_cols  = list(df_metrics.columns[1:4])\n",
    "\n",
    "    # Keep the keys (so we can align rows across bootstrap iterations)\n",
    "    # and the @k metric columns only\n",
    "    return df_metrics[key_cols + at_k_cols]\n",
    "\n",
    "\n",
    "def bootstrap_ci_for_k_metrics(num_features,\n",
    "                               prediction_prob,\n",
    "                               labels,\n",
    "                               fairness_features_df,\n",
    "                               n_iterations=1_000,\n",
    "                               alpha=0.05,\n",
    "                               sample_proportion=1.0,\n",
    "                               random_state=42,\n",
    "                               **calc_kwargs):\n",
    "    \"\"\"\n",
    "    Return a DataFrame whose rows mirror those of `calc_metrics_at_thresholds`\n",
    "    and whose columns give point estimate, variance and CI bounds for every\n",
    "    metric that contains '@k'.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # --- 1. run once on the *full* data → point estimate -------------------\n",
    "    full_df = _fairness_metrics_one_sample(\n",
    "        num_features, prediction_prob, labels, fairness_features_df,\n",
    "        **calc_kwargs\n",
    "    )\n",
    "\n",
    "    key_cols  = [c for c in full_df.columns if '@k' not in c]\n",
    "    metric_cols = [c for c in full_df.columns if '@k' in c]\n",
    "\n",
    "    # keep point estimates for later merge\n",
    "    summary = full_df[key_cols].copy()\n",
    "    # for col in metric_cols:\n",
    "    #     summary[f'{col}_point'] = full_df[col]\n",
    "\n",
    "    # --- 2. bootstrap loop -------------------------------------------------\n",
    "    boot_tables = []\n",
    "    for _ in range(n_iterations):\n",
    "        # resample row indices *with replacement*\n",
    "        n_rows   = len(labels)\n",
    "        n_samp   = int(n_rows * sample_proportion)\n",
    "        idx_boot = rng.choice(n_rows, size=n_samp, replace=True)\n",
    "\n",
    "        boot_pred   = prediction_prob[idx_boot]\n",
    "        boot_labels = labels[idx_boot]\n",
    "        boot_feat   = fairness_features_df.iloc[idx_boot].reset_index(drop=True)\n",
    "        boot_df = _fairness_metrics_one_sample(\n",
    "            num_features, boot_pred, boot_labels, boot_feat,\n",
    "            **calc_kwargs\n",
    "        )\n",
    "        boot_tables.append(boot_df[metric_cols])\n",
    "\n",
    "    # shape → (n_iter, n_rows, n_metrics)\n",
    "    boot_stack = np.stack([t.values for t in boot_tables])\n",
    "\n",
    "    # --- 3. variance and CI per row / per metric ---------------------------\n",
    "    means = boot_stack.mean(axis=0)\n",
    "    var_vals = boot_stack.var(axis=0, ddof=1)\n",
    "    lower_q  = np.quantile(boot_stack, alpha / 2, axis=0)\n",
    "    upper_q  = np.quantile(boot_stack, 1 - alpha / 2, axis=0)\n",
    "\n",
    "    # attach to summary DataFrame\n",
    "    for j, col in enumerate(metric_cols):\n",
    "        summary[f'{col}_mean'] = means[:, j]\n",
    "        summary[f'{col}_var']  = var_vals[:, j]\n",
    "        summary[f'{col}_ci']   = list(zip(lower_q[:, j], upper_q[:, j]))\n",
    "    return summary, boot_tables, metric_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe22d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_bootstrap_dists(\n",
    "    summary,\n",
    "    boot_tables,\n",
    "    metric_cols,\n",
    "    row_idx,\n",
    "    bins=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot bootstrap histograms *per metric* for **one specific row** (setting).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    boot_tables : list[pd.DataFrame]\n",
    "        One DataFrame per bootstrap iteration, rows = settings, cols = metrics.\n",
    "    metric_cols : list[str]\n",
    "        Metrics you want to inspect (e.g. the '@k' columns).\n",
    "    row_idx : int\n",
    "        Index of the row / setting to inspect (same row order as in boot_tables).\n",
    "    bins : int, default 30\n",
    "        Histogram bin count.\n",
    "    \"\"\"\n",
    "    for metric in metric_cols:\n",
    "        # collect the metric’s value for this row across all iterations\n",
    "        samples = np.array([tbl.iloc[row_idx][metric] for tbl in boot_tables])\n",
    "        title = f\"feature-{summary.iloc[row_idx,1:3].iloc[0]}, split-{summary.iloc[row_idx,1:3].iloc[1]}\"\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        plt.hist(samples, bins=bins, color='skyblue', edgecolor='black')\n",
    "        plt.axvline(samples.mean(), color='red', ls='--', lw=1, label='mean')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('value')\n",
    "        plt.ylabel('count')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def line_plot_at_k(\n",
    "        metric_frame,\n",
    "        metric_cols=('prevalence_rate1@k', 'prevalence_rate0@k',\n",
    "                     'treatment_rate1@k',  'treatment_rate0@k')):\n",
    "    \"\"\"\n",
    "    以 k 为横坐标，分别为每个 fairness 切片画多条指标曲线\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 判断 split 列名\n",
    "    split_col = (\"cumulative_split_point(<= split)\"\n",
    "                 if \"cumulative_split_point(<= split)\" in metric_frame.columns\n",
    "                 else \"exact_split_point(== split)\")\n",
    "\n",
    "    id_cols = ['num_features', 'threshold', 'fairness_feature', split_col]\n",
    "\n",
    "    metric_frame = metric_frame.sort_values('k')\n",
    "\n",
    "    for key, df_sub in metric_frame.groupby(id_cols):\n",
    "        num_feat, thresh, feat, split_val = key\n",
    "        title_prefix = (f\"num_features={num_feat}, thr={thresh}, \"\n",
    "                        f\"{feat}={split_val}\")\n",
    "\n",
    "        for key, df_sub in metric_frame.groupby(id_cols):\n",
    "            num_feat, thresh, feat, split_val = key\n",
    "            title_prefix = (f\"num_features={num_feat}, thr={thresh}, \"\n",
    "                            f\"{feat}={split_val}\")\n",
    "\n",
    "            for metric in metric_cols:\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                ax.plot(df_sub['k'], df_sub[metric], marker='o')\n",
    "                ax.set_title(f\"{title_prefix}\\n{metric} vs k\")\n",
    "                ax.set_xlabel('k')\n",
    "                ax.set_ylabel(metric)\n",
    "                ax.grid(True, linestyle='--', alpha=0.4)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        # for key, df_sub in metric_frame.groupby(id_cols):\n",
    "        #     num_feat, thr, feat, split_val = key\n",
    "        #     title = (f\"num_features={num_feat}, thr={thr}, \"\n",
    "        #             f\"{feat}={split_val}\")\n",
    "\n",
    "        #     fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        #     for metric in metric_cols:\n",
    "        #         ax.plot(df_sub['k'], df_sub[metric],\n",
    "        #                 marker='o', label=metric)\n",
    "\n",
    "        #     ax.set_title(title)\n",
    "        #     ax.set_xlabel('k')\n",
    "        #     ax.set_xticks(k_values)\n",
    "        #     ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        #     ax.legend()\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "df= pd.read_csv('Fake_fairness-at-k_metrics.csv')\n",
    "line_plot_at_k(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fa047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "\n",
    "def omnibus_proportion_test(df,\n",
    "                            metric_col,\n",
    "                            split_col=\"exact_split_point(== split)\",\n",
    "                            k_col=\"k\",\n",
    "                            alpha=0.05,\n",
    "                            simulate_iter=10_000,\n",
    "                            seed=0):\n",
    "    \"\"\"\n",
    "    Omnibus H0: the metric is the same across all split levels *within*\n",
    "                each fairness feature.\n",
    "    Works for any metric that is literally   successes / denominator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame with columns\n",
    "        fairness_feature, statistic, dof, p_value, method\n",
    "    \"\"\"\n",
    "    rng   = np.random.default_rng(seed)\n",
    "    rows  = []\n",
    "\n",
    "    for feature, sub in df.groupby(\"fairness_feature\"):\n",
    "        # ---------- build the r × 2 table -------------------------------\n",
    "        table = []\n",
    "        for _, row in sub.iterrows():\n",
    "            if \"prevalence_rate1@k\" == metric_col:                 \n",
    "                denom = row['group_1_sizek']\n",
    "            else:                                 # denominator is group_1_size\n",
    "                denom = row['group_1_size']\n",
    "\n",
    "            succ   = row[metric_col] * denom\n",
    "            fail   = denom - succ\n",
    "            table.append([succ, fail])\n",
    "\n",
    "        table = np.asarray(table, dtype=float)     # (r × 2)\n",
    "\n",
    "        # ---------- decide which test to run ----------------------------\n",
    "        # 2 levels  → Fisher if a cell < 5, else χ²\n",
    "        # ≥3 levels → χ²; if any expected <5 → χ² with Monte-Carlo p-value\n",
    "        method = \"chi2\"\n",
    "        dof    = table.shape[0] - 1\n",
    "\n",
    "        if table.shape[0] == 2:\n",
    "            # 2×2 case\n",
    "            exp = chi2_contingency(table, correction=False)[3]\n",
    "            if (exp < 5).any():\n",
    "                stat, p = fisher_exact(table, alternative=\"two-sided\")\n",
    "                method  = \"fisher_exact\"\n",
    "                dof     = np.nan\n",
    "            else:\n",
    "                stat, p, _, _ = chi2_contingency(table, correction=False)\n",
    "        else:\n",
    "            # r×2, r > 2\n",
    "            stat, p, _, exp = chi2_contingency(table, correction=False)\n",
    "            if (exp < 5).any():\n",
    "                # χ² with Monte-Carlo simulated p (no Yates correction)\n",
    "                stat, p, _, _ = chi2_contingency(\n",
    "                    table, correction=False, lambda_=None,\n",
    "                    simulate_pval=True, random_state=rng, MonteCarlo=simulate_iter\n",
    "                )\n",
    "                method = f\"chi2_sim_{simulate_iter}\"\n",
    "\n",
    "        rows.append({\n",
    "            \"fairness_feature\": feature,\n",
    "            \"statistic\":        stat,\n",
    "            \"dof\":              dof,\n",
    "            \"p_value\":          p,\n",
    "            \"method\":           method,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
